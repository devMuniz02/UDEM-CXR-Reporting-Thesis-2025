{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdd04dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057f8f5de6974199800302adb4305677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emman\\Desktop\\PROYECTOS_VS_CODE\\PRUEBAS_DE_PYTHON\\DINOv3PEF\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\emman\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1079e06d89a445d3aa71c607db62a59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ad1337f95346f1a5969024b588d653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ff1116a0fa4b32b3eea2116562aac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report 1: powerful powerful powerful powerfulaithaithaithaithaithaithaithaithaithaithaithaithaithaithaithaith\n",
      "Report 2: cornerback cornerback cornerback cornerback cornerback cornerback cornerback cornerback cornerback cornerback cornerback cornerback cornerback cornerback cornerback cornerback cornerback cornerback 141 141\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install torch transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Parameters\n",
    "BATCH_SIZE = 2\n",
    "ENCODING_DIM = 384\n",
    "REPORT_MAX_LEN = 20\n",
    "VOCAB_MODEL = \"distilbert-base-uncased\"\n",
    "\n",
    "# Mock image encoding data\n",
    "image_encodings = torch.randn(BATCH_SIZE, ENCODING_DIM)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(VOCAB_MODEL)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# BiLSTM Decoder Model\n",
    "class BiLSTMReportGenerator(nn.Module):\n",
    "    def __init__(self, encoding_dim, hidden_dim, vocab_size, max_len):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.bilstm = nn.LSTM(encoding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "\n",
    "    def forward(self, image_encoding):\n",
    "        # Expand image_encoding to sequence for LSTM input\n",
    "        # (batch, seq_len, encoding_dim)\n",
    "        x = image_encoding.unsqueeze(1).repeat(1, self.max_len, 1)\n",
    "        lstm_out, _ = self.bilstm(x)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "# Instantiate model\n",
    "hidden_dim = 256\n",
    "model = BiLSTMReportGenerator(ENCODING_DIM, hidden_dim, vocab_size, REPORT_MAX_LEN)\n",
    "\n",
    "# Generate mock output\n",
    "with torch.no_grad():\n",
    "    logits = model(image_encodings)\n",
    "    # Get predicted token IDs\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    # Decode to text\n",
    "    reports = []\n",
    "    for ids in predicted_ids:\n",
    "        tokens = tokenizer.convert_ids_to_tokens(ids.tolist())\n",
    "        report = tokenizer.convert_tokens_to_string(tokens)\n",
    "        reports.append(report)\n",
    "\n",
    "# Print generated reports\n",
    "for i, report in enumerate(reports):\n",
    "    print(f\"Report {i+1}: {report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf6ae929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.336823463439941\n",
      "Report 1: situ diving kurt 36thب gonzales haste laundry rd manufacturingroll rivalry issuing moroccan secrecy likely illustrator pere cobb contractors\n",
      "Report 2: teased delgado security asks ag poorly realhus minnesotawani liberation nebraska speech slices festival slices martyutive surgeon cooking\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# --- ensure special tokens ---\n",
    "specials = {}\n",
    "if tokenizer.bos_token is None: specials[\"bos_token\"] = \"<bos>\"\n",
    "if tokenizer.eos_token is None: specials[\"eos_token\"] = \"<eos>\"\n",
    "if tokenizer.pad_token is None: specials[\"pad_token\"] = \"<pad>\"\n",
    "if specials:\n",
    "    tokenizer.add_special_tokens(specials)\n",
    "vocab_size = len(tokenizer)  # update size with new specials\n",
    "\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "EOS_ID = tokenizer.eos_token_id\n",
    "\n",
    "# --- Autoregressive image-conditioned LSTM decoder ---\n",
    "class LSTMReportGenerator(nn.Module):\n",
    "    def __init__(self, encoding_dim, hidden_dim, vocab_size, emb_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        emb_dim = emb_dim or hidden_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_ID)\n",
    "\n",
    "        # init hidden & cell from image encoding\n",
    "        self.init_h = nn.Linear(encoding_dim, hidden_dim)\n",
    "        self.init_c = nn.Linear(encoding_dim, hidden_dim)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(emb_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, img_enc, tgt_ids, teacher_forcing=0.5):\n",
    "        \"\"\"\n",
    "        img_enc: (B, D)\n",
    "        tgt_ids: (B, T)  where tgt[:,0] == BOS and sequence ends with EOS (+ PAD)\n",
    "        returns logits for next-token prediction: (B, T-1, V)\n",
    "        \"\"\"\n",
    "        B, T = tgt_ids.size()\n",
    "        h = torch.tanh(self.init_h(img_enc))\n",
    "        c = torch.tanh(self.init_c(img_enc))\n",
    "\n",
    "        logits = []\n",
    "        y_t = tgt_ids[:, 0]  # BOS\n",
    "        for t in range(1, T):\n",
    "            emb = self.embed(y_t)\n",
    "            h, c = self.lstm(self.drop(emb), (h, c))\n",
    "            step_logits = self.out(self.drop(h))        # (B, V)\n",
    "            logits.append(step_logits.unsqueeze(1))\n",
    "            # scheduled sampling\n",
    "            if random.random() < teacher_forcing:\n",
    "                y_t = tgt_ids[:, t]\n",
    "            else:\n",
    "                y_t = step_logits.argmax(dim=-1)\n",
    "        return torch.cat(logits, dim=1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, img_enc, max_len=50):\n",
    "        \"\"\"\n",
    "        Greedy decoding from BOS until EOS or max_len.\n",
    "        img_enc: (B, D)\n",
    "        returns token ids: (B, L<=max_len+1) including BOS..EOS\n",
    "        \"\"\"\n",
    "        B = img_enc.size(0)\n",
    "        h = torch.tanh(self.init_h(img_enc))\n",
    "        c = torch.tanh(self.init_c(img_enc))\n",
    "        y_t = torch.full((B,), BOS_ID, dtype=torch.long, device=img_enc.device)\n",
    "\n",
    "        seq = [y_t.unsqueeze(1)]\n",
    "        for _ in range(max_len):\n",
    "            emb = self.embed(y_t)\n",
    "            h, c = self.lstm(emb, (h, c))\n",
    "            step_logits = self.out(h)\n",
    "            y_t = step_logits.argmax(dim=-1)\n",
    "            seq.append(y_t.unsqueeze(1))\n",
    "            if (y_t == EOS_ID).all():\n",
    "                break\n",
    "        return torch.cat(seq, dim=1)\n",
    "\n",
    "# --- instantiate ---\n",
    "hidden_dim = 256\n",
    "model = LSTMReportGenerator(ENCODING_DIM, hidden_dim, vocab_size)\n",
    "\n",
    "# --- demo: build dummy target batch for training (BOS ... EOS padded) ---\n",
    "def make_dummy_targets(texts, max_len):\n",
    "    ids = []\n",
    "    for t in texts:\n",
    "        core = tokenizer.encode(t, add_special_tokens=False)\n",
    "        arr = [BOS_ID] + core[:max_len-2] + [EOS_ID]\n",
    "        arr += [PAD_ID] * (max_len - len(arr))\n",
    "        ids.append(arr)\n",
    "    return torch.tensor(ids)\n",
    "\n",
    "dummy_texts = [\"no acute cardiopulmonary process.\",\n",
    "               \"heart size normal. lungs clear.\"]\n",
    "tgt_ids = make_dummy_targets(dummy_texts, REPORT_MAX_LEN)\n",
    "\n",
    "# --- forward (training) ---\n",
    "logits = model(image_encodings, tgt_ids, teacher_forcing=0.7)\n",
    "# cross-entropy ignoring PAD\n",
    "loss = F.cross_entropy(\n",
    "    logits.reshape(-1, logits.size(-1)),\n",
    "    tgt_ids[:, 1:].reshape(-1),\n",
    "    ignore_index=PAD_ID,\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "print(\"Loss:\", float(loss))\n",
    "\n",
    "# --- generate (inference) ---\n",
    "with torch.no_grad():\n",
    "    pred_ids = model.generate(image_encodings, max_len=REPORT_MAX_LEN)\n",
    "    for i, ids in enumerate(pred_ids.tolist()):\n",
    "        print(f\"Report {i+1}:\", tokenizer.decode(ids, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56603dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 10.329631805419922\n",
      "Report 1: adjustments bahamas prices fishingrod refurbishedᵗ sympathyigen こ fide rihanna chevalieriavent overgrown icelandic icelandic officelating illustration narrativeyne mammals romeo installed wreckage impressiverop ま\n",
      "Report 2: adjustments castroyan dissent institutes furiously determine ע huntsvilledomsokan mythological message mc nurses dumont accountability mt [unused470] mass disaster 269 pistonzilyverevere swell bei新\n"
     ]
    }
   ],
   "source": [
    "# pip install torch transformers\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "\n",
    "# ---------------- tokenizer & specials ----------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "specials = {}\n",
    "if tokenizer.bos_token is None: specials[\"bos_token\"] = \"<bos>\"\n",
    "if tokenizer.eos_token is None: specials[\"eos_token\"] = \"<eos>\"\n",
    "if tokenizer.pad_token is None: specials[\"pad_token\"] = \"<pad>\"\n",
    "if specials: tokenizer.add_special_tokens(specials)\n",
    "\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "BOS_ID = tokenizer.bos_token_id\n",
    "EOS_ID = tokenizer.eos_token_id\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "\n",
    "# ---------------- LSTM encoder-decoder ----------------\n",
    "class BiLSTMImageEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, num_layers=1, dropout=0.1,\n",
    "                 use_patient=False, patient_dim=0):\n",
    "        super().__init__()\n",
    "        self.use_patient = use_patient and patient_dim > 0\n",
    "        self.patient_proj = nn.Linear(patient_dim, in_dim) if self.use_patient else None\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=in_dim, hidden_size=hidden, num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, img_tokens, patient_feats=None):\n",
    "        # img_tokens: (B, L=96, D=96)\n",
    "        if self.use_patient:\n",
    "            assert patient_feats is not None\n",
    "            pt = self.patient_proj(patient_feats).unsqueeze(1)      # (B,1,D)\n",
    "            x = torch.cat([pt, img_tokens], dim=1)                  # (B, L+1, D)\n",
    "        else:\n",
    "            x = img_tokens\n",
    "        enc_out, _ = self.bilstm(self.dropout(x))                   # (B, L', 2H)\n",
    "        return enc_out\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, dec_hidden, enc_dim, attn_dim=256):\n",
    "        super().__init__()\n",
    "        self.W_h, self.W_s = nn.Linear(enc_dim, attn_dim, False), nn.Linear(dec_hidden, attn_dim, False)\n",
    "        self.v = nn.Linear(attn_dim, 1, False)\n",
    "\n",
    "    def forward(self, enc_out, s_t):                               # enc_out: (B,L,E), s_t: (B,H)\n",
    "        e = self.v(torch.tanh(self.W_h(enc_out) + self.W_s(s_t).unsqueeze(1)))  # (B,L,1)\n",
    "        a = torch.softmax(e, dim=1)                                # (B,L,1)\n",
    "        ctx = (a * enc_out).sum(1)                                 # (B,E)\n",
    "        return ctx, a.squeeze(-1)\n",
    "\n",
    "class AttnLSTMDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, dec_hidden, enc_dim, dropout=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTMCell(emb_dim + enc_dim, dec_hidden)\n",
    "        self.attn = AdditiveAttention(dec_hidden, enc_dim)\n",
    "        self.out  = nn.Linear(dec_hidden + enc_dim, vocab_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, enc_out, tgt, bos_id, eos_id, teacher_forcing=0.5):\n",
    "        B, T = tgt.size()\n",
    "        H = self.lstm.hidden_size\n",
    "        h = enc_out.mean(1).new_zeros((B, H))\n",
    "        c = enc_out.mean(1).new_zeros((B, H))\n",
    "        y = tgt[:, 0]                                              # BOS\n",
    "        logits = []\n",
    "        for t in range(1, T):\n",
    "            emb = self.emb(y)\n",
    "            ctx, _ = self.attn(enc_out, h)\n",
    "            h, c = self.lstm(self.drop(torch.cat([emb, ctx], -1)), (h, c))\n",
    "            step = self.out(self.drop(torch.cat([h, ctx], -1)))\n",
    "            logits.append(step.unsqueeze(1))\n",
    "            y = tgt[:, t] if random.random() < teacher_forcing else step.argmax(-1)\n",
    "        return torch.cat(logits, 1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, enc_out, bos_id, eos_id, max_len=60):\n",
    "        B = enc_out.size(0); H = self.lstm.hidden_size\n",
    "        h = enc_out.mean(1).new_zeros((B, H)); c = enc_out.mean(1).new_zeros((B, H))\n",
    "        y = torch.full((B,), bos_id, dtype=torch.long, device=enc_out.device)\n",
    "        seq = [y.unsqueeze(1)]\n",
    "        for _ in range(max_len):\n",
    "            emb = self.emb(y)\n",
    "            ctx, _ = self.attn(enc_out, h)\n",
    "            h, c = self.lstm(torch.cat([emb, ctx], -1), (h, c))\n",
    "            y = self.out(torch.cat([h, ctx], -1)).argmax(-1)\n",
    "            seq.append(y.unsqueeze(1))\n",
    "            if (y == eos_id).all(): break\n",
    "        return torch.cat(seq, 1)\n",
    "\n",
    "class ImageToReport(nn.Module):\n",
    "    def __init__(self, img_feat_dim=96, enc_hidden=256, emb_dim=256, dec_hidden=512,\n",
    "                 vocab_size=VOCAB_SIZE, pad_idx=PAD_ID, use_patient=False, patient_dim=0):\n",
    "        super().__init__()\n",
    "        self.encoder = BiLSTMImageEncoder(img_feat_dim, enc_hidden//2,\n",
    "                                          num_layers=1, dropout=0.1,\n",
    "                                          use_patient=use_patient, patient_dim=patient_dim)\n",
    "        self.decoder = AttnLSTMDecoder(vocab_size, emb_dim, dec_hidden,\n",
    "                                       enc_dim=enc_hidden, dropout=0.1, pad_idx=pad_idx)\n",
    "\n",
    "    def forward(self, img_tokens, tgt_ids, patient_feats=None, tf=0.5):\n",
    "        enc = self.encoder(img_tokens, patient_feats)              # (B, L', enc_hidden)\n",
    "        return self.decoder(enc, tgt_ids, BOS_ID, EOS_ID, teacher_forcing=tf)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, img_tokens, patient_feats=None, max_len=60):\n",
    "        enc = self.encoder(img_tokens, patient_feats)\n",
    "        return self.decoder.generate(enc, BOS_ID, EOS_ID, max_len=max_len)\n",
    "\n",
    "# ---------------- demo with your shape (B, 96, 96) ----------------\n",
    "BATCH = 2\n",
    "IMG_TOKENS = torch.randn(BATCH, 96, 96)      # (B, L=96, D=96) from DINO\n",
    "model = ImageToReport(img_feat_dim=96, enc_hidden=256, emb_dim=256, dec_hidden=512)\n",
    "\n",
    "def pack_targets(texts, max_len=40):\n",
    "    ids = []\n",
    "    for t in texts:\n",
    "        core = tokenizer.encode(t, add_special_tokens=False)\n",
    "        arr = [BOS_ID] + core[:max_len-2] + [EOS_ID]\n",
    "        arr += [PAD_ID] * (max_len - len(arr))\n",
    "        ids.append(arr)\n",
    "    return torch.tensor(ids)\n",
    "\n",
    "targets = pack_targets(\n",
    "    [\"no acute cardiopulmonary process.\",\n",
    "     \"heart size normal. lungs clear.\"],\n",
    "    max_len=30\n",
    ")\n",
    "\n",
    "# training step\n",
    "logits = model(IMG_TOKENS, targets, tf=0.7)                      # (B, T-1, V)\n",
    "loss = F.cross_entropy(\n",
    "    logits.reshape(-1, logits.size(-1)),\n",
    "    targets[:, 1:].reshape(-1), ignore_index=PAD_ID, label_smoothing=0.1\n",
    ")\n",
    "print(\"loss:\", float(loss))\n",
    "\n",
    "# inference\n",
    "with torch.no_grad():\n",
    "    pred_ids = model.generate(IMG_TOKENS, max_len=10)\n",
    "    for i, seq in enumerate(pred_ids.tolist()):\n",
    "        print(f\"Report {i+1}:\", tokenizer.decode(seq, skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e3d2fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report 1: adjustments regardless bazaar variouslyvishiser jp vegasulin robotic\n",
      "Report 2: adjustments castroyan dissent institutes furiouslylastic cox furiously с\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred_ids = model.generate(IMG_TOKENS, max_len=10)\n",
    "    for i, seq in enumerate(pred_ids.tolist()):\n",
    "        print(f\"Report {i+1}:\", tokenizer.decode(seq, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "350267fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n",
      "Specials -> pad: 0 cls: 101 sep: 102\n",
      "Using device: cuda\n",
      "\n",
      "=== Training (CLS encodings) ===\n",
      "[CLS] epoch 1/2 | train CE/token=10.3267 | val CE/token=10.2007\n",
      "[CLS] epoch 2/2 | train CE/token=10.1533 | val CE/token=10.0339\n",
      "[CLS GEN 0]: a ∧ cinema orrclaveclaveletteclaveclave praising lobstershedcek blown also [unused482] solo canadians technicians 03\n",
      "[CLS GEN 1]: a a pedersen keyscts leaders territorial workings mp3 slacksᵈiary taskedchment conservatoireـ snakes spiked [unused444] ventral\n",
      "[CLS GEN 2]: a a docked riding riding nakamura chefs adolfllis bruises commandoieri exhaustion humanities reasonably piracy aroma catholicmobilebuilding\n",
      "[CLS GEN 3]: a group ħ operator laboratories laboratories collaborating opponents jed withdrawing 71 topicnkaeseildeん catchment harcourt octopussari\n",
      "\n",
      "=== Training (PATCH encodings) ===\n",
      "[PATCH] epoch 1/2 | train CE/token=10.2271 | val CE/token=10.1419\n",
      "[PATCH] epoch 2/2 | train CE/token=10.0984 | val CE/token=9.9923\n",
      "[PATCH GEN 0]: a a\n",
      "[PATCH GEN 1]: a a\n",
      "[PATCH GEN 2]: a a\n",
      "[PATCH GEN 3]: a a\n"
     ]
    }
   ],
   "source": [
    "# If you don't have transformers installed:\n",
    "# !pip install -q transformers\n",
    "\n",
    "import math\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.__version__\n",
    "def get_tokenizer():\n",
    "    tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    assert tok.pad_token_id is not None, \"BERT tokenizer must have a pad_token_id.\"\n",
    "    return tok\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Specials -> pad:\", tokenizer.pad_token_id, \"cls:\", tokenizer.cls_token_id, \"sep:\", tokenizer.sep_token_id)\n",
    "class EncodedCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps precomputed encodings and text captions.\n",
    "\n",
    "    mode:\n",
    "      - 'cls'   : x is tensor of shape (384,)\n",
    "      - 'patch' : x is tensor of shape (T=96, D=96)\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings: List[torch.Tensor], captions: List[str], tokenizer, mode: str):\n",
    "        assert mode in (\"cls\", \"patch\"), \"mode must be 'cls' or 'patch'\"\n",
    "        assert len(encodings) == len(captions), \"encodings and captions length mismatch\"\n",
    "        self.encodings = encodings\n",
    "        self.captions = captions\n",
    "        self.tok = tokenizer\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.encodings[idx]\n",
    "        text = self.captions[idx]\n",
    "        # Encode without adding specials here; we add [CLS]/[SEP] in collate\n",
    "        ids = self.tok.encode(text, add_special_tokens=False)\n",
    "        return x, torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "def collate_fn_bert(batch, tokenizer):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      encs: stacked encodings\n",
    "         - if CLS mode: (B, 384)\n",
    "         - if PATCH mode: (B, T, D)\n",
    "      caps_in: (B, T_in)   with [CLS] prefix\n",
    "      caps_out: (B, T_out) with [SEP] suffix\n",
    "      lengths_out: (B,) integer lengths for targets (non-padded)\n",
    "      mode: 'cls' or 'patch' inferred from the first sample\n",
    "    \"\"\"\n",
    "    encs, caps = zip(*batch)\n",
    "    mode = \"cls\" if encs[0].dim() == 1 else \"patch\"\n",
    "\n",
    "    if mode == \"cls\":\n",
    "        encs = torch.stack(encs, dim=0)      # (B, 384)\n",
    "    else:\n",
    "        encs = torch.stack(encs, dim=0)      # (B, T, D) (T=D=96 by default mock)\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    sos_id = tokenizer.cls_token_id  # use [CLS] as SOS\n",
    "    eos_id = tokenizer.sep_token_id  # use [SEP] as EOS\n",
    "\n",
    "    caps_in, caps_out, lengths_out = [], [], []\n",
    "    for c in caps:\n",
    "        c_in = torch.cat([torch.tensor([sos_id]), c], dim=0)      # [CLS] + tokens\n",
    "        c_out = torch.cat([c, torch.tensor([eos_id])], dim=0)     # tokens + [SEP]\n",
    "        caps_in.append(c_in)\n",
    "        caps_out.append(c_out)\n",
    "        lengths_out.append(len(c_out))\n",
    "\n",
    "    caps_in  = pad_sequence(caps_in,  batch_first=True, padding_value=pad_id)\n",
    "    caps_out = pad_sequence(caps_out, batch_first=True, padding_value=pad_id)\n",
    "    lengths_out = torch.tensor(lengths_out, dtype=torch.long)\n",
    "\n",
    "    return encs, caps_in, caps_out, lengths_out, mode\n",
    "class AttnPool1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight attention pooling across token sequence.\n",
    "    Input: (B, T, Din) -> Output: (B, Dout)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_hidden: int, d_out: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_in, d_hidden)\n",
    "        self.u = nn.Parameter(torch.randn(d_hidden))\n",
    "        self.out = nn.Linear(d_in, d_out)\n",
    "\n",
    "    def forward(self, x):  # (B, T, Din)\n",
    "        scores = torch.tanh(self.proj(x)) @ self.u     # (B, T)\n",
    "        w = scores.softmax(dim=1).unsqueeze(-1)        # (B, T, 1)\n",
    "        pooled = (x * w).sum(dim=1)                    # (B, Din)\n",
    "        return self.out(pooled)                        # (B, Dout)\n",
    "\n",
    "class DINOv3Adapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - CLS vector (B, 384)\n",
    "      - Patch tokens (B, T=96, D=96)\n",
    "    Produces a single image embedding (B, E).\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size: int, cls_dim: int = 384, patch_dim: int = 96):\n",
    "        super().__init__()\n",
    "        self.proj_cls = nn.Linear(cls_dim, embed_size)\n",
    "        self.proj_patch = nn.Linear(patch_dim, embed_size)\n",
    "        self.pool = AttnPool1D(d_in=embed_size, d_hidden=embed_size, d_out=embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:          # (B, 384)\n",
    "            return self.proj_cls(x)\n",
    "        elif x.dim() == 3:        # (B, T, D)\n",
    "            z = self.proj_patch(x)  # (B, T, E)\n",
    "            return self.pool(z)     # (B, E)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected DINOv3 encoding shape: {tuple(x.shape)}\")\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int,\n",
    "                 pad_id: int, num_layers: int = 1, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.word_embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad_id)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=(dropout if num_layers > 1 else 0.0))\n",
    "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
    "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
    "        self.ctx_bias = nn.Linear(embed_size, embed_size)  # per-step bias from image vec\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_vec, captions_in, lengths_out):\n",
    "        \"\"\"\n",
    "        img_vec: (B, E)\n",
    "        captions_in: (B, T_in) with [CLS] at t=0\n",
    "        lengths_out: (B,) lengths for targets (tokens + [SEP])\n",
    "        \"\"\"\n",
    "        h0 = torch.tanh(self.init_h(img_vec)).unsqueeze(0)  # (1, B, H)\n",
    "        c0 = torch.tanh(self.init_c(img_vec)).unsqueeze(0)  # (1, B, H)\n",
    "\n",
    "        emb = self.word_embed(captions_in)                  # (B, T, E)\n",
    "        bias = self.ctx_bias(img_vec).unsqueeze(1)          # (B, 1, E)\n",
    "        emb = emb + bias\n",
    "\n",
    "        packed = pack_padded_sequence(emb, lengths_out.tolist(),\n",
    "                                      batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed, (h0, c0))\n",
    "        logits = self.fc(packed_out.data)                   # (sumT, V)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, img_vec, max_len: int, sos_id: int, eos_id: int) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Greedy decode (no beam). Returns token id lists without the SOS token.\n",
    "        \"\"\"\n",
    "        device = img_vec.device\n",
    "        B = img_vec.size(0)\n",
    "        h = torch.tanh(self.init_h(img_vec)).unsqueeze(0)\n",
    "        c = torch.tanh(self.init_c(img_vec)).unsqueeze(0)\n",
    "        bias = self.ctx_bias(img_vec).unsqueeze(1)          # (B, 1, E)\n",
    "\n",
    "        inputs = torch.full((B, 1), sos_id, dtype=torch.long, device=device)\n",
    "        out_ids = [[] for _ in range(B)]\n",
    "        for _ in range(max_len):\n",
    "            emb = self.word_embed(inputs) + bias            # (B, 1, E)\n",
    "            y, (h, c) = self.lstm(emb, (h, c))              # (B, 1, H)\n",
    "            logits = self.fc(y.squeeze(1))                  # (B, V)\n",
    "            nxt = torch.argmax(logits, dim=-1)              # (B,)\n",
    "            for i in range(B):\n",
    "                out_ids[i].append(nxt[i].item())\n",
    "            inputs = nxt.unsqueeze(1)\n",
    "\n",
    "        # Trim at EOS if present\n",
    "        trimmed = []\n",
    "        for seq in out_ids:\n",
    "            trimmed.append(seq[: seq.index(eos_id)] if eos_id in seq else seq)\n",
    "        return trimmed\n",
    "\n",
    "class DINOv3Captioner(nn.Module):\n",
    "    def __init__(self, tokenizer, embed_size=256, hidden_size=512, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.tok = tokenizer\n",
    "        self.adapter = DINOv3Adapter(embed_size=embed_size, cls_dim=384, patch_dim=96)\n",
    "        self.decoder = LSTMDecoder(\n",
    "            vocab_size=self.tok.vocab_size,\n",
    "            embed_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            pad_id=self.tok.pad_token_id,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, dino_enc, caps_in, lengths_out):\n",
    "        img_vec = self.adapter(dino_enc)  # (B, E)\n",
    "        return self.decoder(img_vec, caps_in, lengths_out)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def caption(self, dino_enc, max_len: int = 30):\n",
    "        img_vec = self.adapter(dino_enc)\n",
    "        ids = self.decoder.generate(img_vec, max_len,\n",
    "                                    sos_id=self.tok.cls_token_id,\n",
    "                                    eos_id=self.tok.sep_token_id)\n",
    "        texts = [self.tok.decode(seq, skip_special_tokens=True,\n",
    "                                 clean_up_tokenization_spaces=True) for seq in ids]\n",
    "        return texts\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device=\"cuda\"):\n",
    "    model.train()\n",
    "    total, denom = 0.0, 0\n",
    "    for encs, caps_in, caps_out, lengths_out, _mode in loader:\n",
    "        encs = encs.to(device)\n",
    "        caps_in = caps_in.to(device)\n",
    "        caps_out = caps_out.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(encs, caps_in, lengths_out)  # (sumT, V)\n",
    "\n",
    "        packed_targets = pack_padded_sequence(caps_out, lengths_out.tolist(),\n",
    "                                              batch_first=True, enforce_sorted=False)\n",
    "        loss = criterion(logits, packed_targets.data)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item() * packed_targets.data.numel()\n",
    "        denom += packed_targets.data.numel()\n",
    "    return total / max(1, denom)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    total, denom = 0.0, 0\n",
    "    for encs, caps_in, caps_out, lengths_out, _mode in loader:\n",
    "        encs = encs.to(device)\n",
    "        caps_in = caps_in.to(device)\n",
    "        caps_out = caps_out.to(device)\n",
    "\n",
    "        logits = model(encs, caps_in, lengths_out)\n",
    "        packed_targets = pack_padded_sequence(caps_out, lengths_out.tolist(),\n",
    "                                              batch_first=True, enforce_sorted=False)\n",
    "        loss = criterion(logits, packed_targets.data)\n",
    "        total += loss.item() * packed_targets.data.numel()\n",
    "        denom += packed_targets.data.numel()\n",
    "    return total / max(1, denom)\n",
    "def make_mock_data_cls(n=16, seed=1337):\n",
    "    random.seed(seed); torch.manual_seed(seed)\n",
    "    captions = [\n",
    "        \"a dog running on grass\",\n",
    "        \"a cat sitting on a couch\",\n",
    "        \"a person riding a bike\",\n",
    "        \"a group of people on the beach\",\n",
    "        \"a car parked on the street\",\n",
    "        \"a child playing with a ball\",\n",
    "        \"a man cooking in a kitchen\",\n",
    "        \"a woman reading a book\",\n",
    "    ]\n",
    "    captions = (captions * ((n + len(captions) - 1)//len(captions)))[:n]\n",
    "    encs = [torch.randn(384) for _ in range(n)]   # CLS vectors\n",
    "    return encs, captions\n",
    "\n",
    "def make_mock_data_patch(n=16, seed=2024):\n",
    "    random.seed(seed); torch.manual_seed(seed)\n",
    "    captions = [\n",
    "        \"a mountain covered with snow\",\n",
    "        \"a river flowing through a forest\",\n",
    "        \"an airplane flying in the sky\",\n",
    "        \"a delicious pizza on a plate\",\n",
    "        \"a soccer player kicking a ball\",\n",
    "        \"a dog catching a frisbee\",\n",
    "        \"a train arriving at a station\",\n",
    "        \"a city skyline at night\",\n",
    "    ]\n",
    "    captions = (captions * ((n + len(captions) - 1)//len(captions)))[:n]\n",
    "    encs = [torch.randn(96, 96) for _ in range(n)]  # Patch tokens (T=96, D=96)\n",
    "    return encs, captions\n",
    "def build_loader(encs, caps, tokenizer, mode: str, batch_size=8, workers=0, shuffle=True):\n",
    "    ds = EncodedCaptionDataset(encs, caps, tokenizer, mode=mode)\n",
    "    collate = lambda b: collate_fn_bert(b, tokenizer)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle,\n",
    "                      num_workers=workers, collate_fn=collate, pin_memory=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Hyperparams (adjust for your real training)\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "epochs = 2\n",
    "batch_size = 8\n",
    "lr = 2e-4\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = DINOv3Captioner(tokenizer, embed_size=embed_size,\n",
    "                        hidden_size=hidden_size, num_layers=num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# ---- CLS mock run ----\n",
    "encs_cls, caps_cls = make_mock_data_cls(n=24)\n",
    "train_loader_cls = build_loader(encs_cls, caps_cls, tokenizer, mode=\"cls\",\n",
    "                                batch_size=batch_size, shuffle=True)\n",
    "val_loader_cls   = build_loader(encs_cls, caps_cls, tokenizer, mode=\"cls\",\n",
    "                                batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"\\n=== Training (CLS encodings) ===\")\n",
    "best_val = math.inf\n",
    "for ep in range(epochs):\n",
    "    tr = train_one_epoch(model, train_loader_cls, optimizer, criterion, device=device)\n",
    "    va = evaluate(model, val_loader_cls, criterion, device=device)\n",
    "    best_val = min(best_val, va)\n",
    "    print(f\"[CLS] epoch {ep+1}/{epochs} | train CE/token={tr:.4f} | val CE/token={va:.4f}\")\n",
    "\n",
    "# Greedy generation\n",
    "batch = next(iter(val_loader_cls))\n",
    "encs_b, caps_in_b, caps_out_b, lengths_b, mode_b = batch\n",
    "pred_texts = model.caption(encs_b[:4].to(device), max_len=20)\n",
    "for i, t in enumerate(pred_texts):\n",
    "    print(f\"[CLS GEN {i}]: {t}\")\n",
    "# Reuse same model/optimizer for brevity; in practice you may want separate runs/checkpoints.\n",
    "encs_patch, caps_patch = make_mock_data_patch(n=24)\n",
    "train_loader_patch = build_loader(encs_patch, caps_patch, tokenizer, mode=\"patch\",\n",
    "                                  batch_size=batch_size, shuffle=True)\n",
    "val_loader_patch   = build_loader(encs_patch, caps_patch, tokenizer, mode=\"patch\",\n",
    "                                  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"\\n=== Training (PATCH encodings) ===\")\n",
    "for ep in range(epochs):\n",
    "    tr = train_one_epoch(model, train_loader_patch, optimizer, criterion, device=device)\n",
    "    va = evaluate(model, val_loader_patch, criterion, device=device)\n",
    "    print(f\"[PATCH] epoch {ep+1}/{epochs} | train CE/token={tr:.4f} | val CE/token={va:.4f}\")\n",
    "\n",
    "# Greedy generation\n",
    "batch = next(iter(val_loader_patch))\n",
    "encs_b, caps_in_b, caps_out_b, lengths_b, mode_b = batch\n",
    "pred_texts = model.caption(encs_b[:4].to(device), max_len=20)\n",
    "for i, t in enumerate(pred_texts):\n",
    "    print(f\"[PATCH GEN {i}]: {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc823ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DINOv3PEF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
