{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a43f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTM_Attn import *\n",
    "from GPT import PadChestGRDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417fdf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[INFO] Kept 47494/223462 rows with existing PNGs under C:\\Users\\emman\\Desktop\\PROYECTOS_VS_CODE\\PRUEBAS_DE_PYTHON\\CheXpertPlus\\PNG\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "valid_df = build_valid_df(CSV_PATH, IMG_ROOT)\n",
    "if valid_df.empty:\n",
    "    print(\"[WARN] No valid rows found; check paths and PNG conversion.\")\n",
    "labels_as_str = valid_df[TEXT_COL].astype(str).tolist()\n",
    "tokenizer = build_tokenizer_from_labels(labels_as_str)\n",
    "pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n",
    "bos_id = getattr(tokenizer, \"bos_token_id\", 1)\n",
    "eos_id = getattr(tokenizer, \"eos_token_id\", 2)\n",
    "tf = dino_image_transform(img_size=516)\n",
    "# ds = CheXpertDataset(img_root=IMG_ROOT, csv=valid_df, transform=tf, text_col=TEXT_COL)\n",
    "\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(r\"C:\\Users\\emman\\Desktop\\PROYECTOS_VS_CODE\\PRUEBAS_DE_PYTHON\\PadChest-GR\\master_table.csv\")   # must contain column 'ImageID'\n",
    "root_dir = r'C:\\Users\\emman\\Desktop\\PROYECTOS_VS_CODE\\PRUEBAS_DE_PYTHON\\PadChest-GR\\PadChest_GR'\n",
    "json_file = r\"C:\\Users\\emman\\Desktop\\PROYECTOS_VS_CODE\\PRUEBAS_DE_PYTHON\\PadChest-GR\\grounded_reports_20240819.json\"\n",
    "\n",
    "# Uncomment and set your variables before running:\n",
    "\n",
    "dataframe=df_train\n",
    "root_dir=root_dir\n",
    "json_file=json_file\n",
    "batch_size=8\n",
    "num_workers=0    # try 0 if on Windows/Colab and you hit issues\n",
    "image_size=1024\n",
    "max_txt_len=64   # None => auto from p95+8; or set e.g. 64\n",
    "sentence_key=\"sentence_en\"\n",
    "return_paths=False\n",
    "\n",
    "\n",
    "# DINO expects 224 or 518 square; 224 is fine here\n",
    "IMG_SIZE = 516\n",
    "tf = dino_image_transform(img_size=IMG_SIZE)\n",
    "ds = PadChestGRDataset(\n",
    "        dataframe=dataframe,\n",
    "        root_dir=root_dir,\n",
    "        json_file=json_file,\n",
    "        # tokenizer=tokenizer,\n",
    "        max_txt_len=max_txt_len,\n",
    "        image_size=image_size,\n",
    "        normalize=True,\n",
    "        transform=None,\n",
    "        return_paths=return_paths,\n",
    "        sentence_key=sentence_key,\n",
    "    )\n",
    "\n",
    "collate_fn = CaptionCollate(tokenizer, pad_id)\n",
    "is_windows = os.name == \"nt\"\n",
    "num_workers = 0 if is_windows else 2\n",
    "persistent_workers = False if num_workers == 0 else True\n",
    "loader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=persistent_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "train_ds = torch.utils.data.Subset(ds, range(0, 80))#int(len(ds)*.8)))\n",
    "valid_ds = torch.utils.data.Subset(ds, range(80, 160))#int(len(ds)*.8), len(ds)))\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "D_IMG = 384\n",
    "model = DinoLSTMAttnCaptioner(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_img=D_IMG,\n",
    "    d_h=512,\n",
    "    pad_id=pad_id,\n",
    "    dino_model_id=\"facebook/dinov3-vits16-pretrain-lvd1689m\",\n",
    "    freeze_dino=True,\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4, weight_decay=1e-2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2ec80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:14<00:00,  1.49s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.5040, PPL=4.51 | Val Loss=5.1955, Val PPL=275.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:14<00:00,  1.45s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=1.4675, PPL=4.34 | Val Loss=5.2106, Val PPL=277.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:14<00:00,  1.49s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=1.4532, PPL=4.28 | Val Loss=5.2303, Val PPL=284.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=1.4325, PPL=4.19 | Val Loss=5.2487, Val PPL=293.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:14<00:00,  1.43s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=1.4322, PPL=4.19 | Val Loss=5.2654, Val PPL=293.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:14<00:00,  1.47s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=1.4168, PPL=4.12 | Val Loss=5.2874, Val PPL=300.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:14<00:00,  1.44s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=1.4132, PPL=4.11 | Val Loss=5.3184, Val PPL=314.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:14<00:00,  1.47s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=1.4062, PPL=4.08 | Val Loss=5.3300, Val PPL=314.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:14<00:00,  1.47s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss=1.4056, PPL=4.08 | Val Loss=5.3294, Val PPL=311.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:15<00:00,  1.50s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss=1.4030, PPL=4.07 | Val Loss=5.3441, Val PPL=316.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:15<00:00,  1.56s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss=1.3991, PPL=4.05 | Val Loss=5.3647, Val PPL=323.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:15<00:00,  1.55s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss=1.3954, PPL=4.04 | Val Loss=5.4029, Val PPL=341.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:14<00:00,  1.50s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss=1.3920, PPL=4.02 | Val Loss=5.4320, Val PPL=348.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:15<00:00,  1.51s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss=1.3903, PPL=4.02 | Val Loss=5.4426, Val PPL=352.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss=1.3886, PPL=4.01 | Val Loss=5.4261, Val PPL=346.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss=1.3866, PPL=4.00 | Val Loss=5.4373, Val PPL=348.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:15<00:00,  1.55s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss=1.3913, PPL=4.02 | Val Loss=5.4420, Val PPL=349.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:15<00:00,  1.53s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss=1.3872, PPL=4.00 | Val Loss=5.4683, Val PPL=358.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss=1.3851, PPL=4.00 | Val Loss=5.5030, Val PPL=374.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\n",
      "Evaluating: 100%|██████████| 10/10 [00:11<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss=1.3836, PPL=3.99 | Val Loss=5.4888, Val PPL=364.62\n",
      "Predictions test:\n",
      "\n",
      "TEST GEN 1: minimal biapical pleural thickening. slight blunting of the posterior left costophrenic angle. no other significant alterations. salman. cellar hillylogy chartered cat dina in the regionulesity. nara increase in the ct scan\n",
      "TEST TARGET 1: right humeral prosthesis.\n",
      "\n",
      "TEST GEN 2: cardiomegaly. aortic calcification. aortic elongation. scoliosis. no other relevant findings. tomas shouted mass detonated paranaored in the music bro retorted basal parks. calcified granuloma.\n",
      "TEST TARGET 2: elongated aorta. hiatal hernia.\n",
      "\n",
      "TEST GEN 3: cardiomegaly. aortic calcification. aortic elongation. scoliosis. no other relevant findings.立 ལ tampa institutional snap relational slowing signatures tarzan owner bone congressman favor debt petitionsroniaմ class. canyon\n",
      "TEST TARGET 3: elongated aorta. hiatal hernia.\n",
      "\n",
      "TEST GEN 4: bilateral apical pleural thickening. no other significant findings. briefcase the subcentis is royale.ing of the right rib cage fracture. prominent hila. stemming probably related to pericardial fat. slight widening of the superior\n",
      "TEST TARGET 4: no infiltrates or consolidations are appreciated. chronic changes in lung parenchyma. cardiothoracic index at the upper limit of normal. mechanical changes in intersomatic spaces.\n",
      "\n",
      "TEST GEN 5: bilateral apical pleural thickening. no other significant findings. flocknum. of the definitely fatty projected kathleen relevant findings. initiative northampton six outputs fellowships in the right middle lung field. tailed mocked traveler infiltrate vince goalscorerren 1925\n",
      "TEST TARGET 5: no infiltrates or consolidations are appreciated. chronic changes in lung parenchyma. cardiothoracic index at the upper limit of normal. mechanical changes in intersomatic spaces.\n",
      "\n",
      "TEST GEN 6: bilateral apical pleural thickening. no other significant findings.adt kw coe内 bombardment atilityct scoliosis. no other relevant findings. agency logos accident wool soviets couldneal ר 27 potato theatrical age infiltrate. morse\n",
      "TEST TARGET 6: no infiltrates or consolidations are appreciated. chronic changes in lung parenchyma. cardiothoracic index at the upper limit of normal. mechanical changes in intersomatic spaces.\n",
      "\n",
      "TEST GEN 7: segmental atelectasis in the lateral segment of the middle lobe. elevation of the right hemidiaphragm. volume loss. prominent right hilum. wedge - shaped deformity of the middle dorsal vertebral body. likely\n",
      "TEST TARGET 7: changes from right breast and axillary surgery. bilateral mammary prostheses. no other significant radiological findings.\n",
      "\n",
      "TEST GEN 8: segmental atelectasis in the lateral segment of the middle lobe. elevation of the right hemidiaphragm. volume loss. prominent right hilum. wedge - shaped deformity of the middle dorsal vertebral body. likely\n",
      "TEST TARGET 8: changes from right breast and axillary surgery. bilateral mammary prostheses. no other significant radiological findings.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    slice_train_loader = islice(train_loader, 10)\n",
    "    slice_valid_loader = islice(valid_loader, 10)\n",
    "    train_stats = train_one_epoch(model, slice_train_loader, optimizer, device, pad_id, num_batches=10, grad_clip=1.0)\n",
    "    val_stats = evaluate(model, slice_valid_loader, device, pad_id, num_batches=10)\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss={train_stats['loss']:.4f}, PPL={train_stats['ppl']:.2f} | \"\n",
    "            f\"Val Loss={val_stats['val_loss']:.4f}, Val PPL={val_stats['val_ppl']:.2f}\")\n",
    "test_loader_sliced = iter(valid_loader)\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_sliced:\n",
    "        pixel_values, ids_loader, paths, raw_labels = batch\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        gen_ids = model.generate(\n",
    "            pixel_values=pixel_values,\n",
    "            bos_id=bos_id, eos_id=eos_id,\n",
    "            max_new_tokens=50, top_p=0.8, temperature=0.9, greedy=False\n",
    "        )\n",
    "        print(\"Predictions test:\")\n",
    "        for i in range(gen_ids.size(0)):\n",
    "            print(f\"\\nTEST GEN {i+1}:\", tokenizer.decode(gen_ids[i].tolist()))\n",
    "            print(f\"TEST TARGET {i+1}:\", tokenizer.decode(ids_loader[i].tolist()))\n",
    "        # Free batch memory\n",
    "        del pixel_values, ids_loader, paths, raw_labels, gen_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd72eb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CheXbert] Deleted old weights at: C:\\Users\\emman\\AppData\\Local\\chexbert\\chexbert\\Cache\\Cache\\chexbert.pth\n",
      "[CheXbert] Downloading weights from: https://stanfordmedicine.box.com/shared/static/c3stck6w6dol3h36grdc97xoydzxd7w9\n",
      "[CheXbert] Download failed from https://stanfordmedicine.box.com/shared/static/c3stck6w6dol3h36grdc97xoydzxd7w9: 404 Client Error: Not Found for url: https://stanfordmedicine.app.box.com/public/static/c3stck6w6dol3h36grdc97xoydzxd7w9\n",
      "[CheXbert] Downloading weights from: https://huggingface.co/StanfordAIMI/RRG_scorers/resolve/main/chexbert.pth\n",
      "[CheXbert] Weights saved to: C:\\Users\\emman\\AppData\\Local\\chexbert\\chexbert\\Cache\\Cache\\chexbert.pth\n",
      "[Info] CheXbert not available: [Errno 2] No such file or directory: 'C:\\\\Users\\\\emman\\\\AppData\\\\Local\\\\chexbert\\\\chexbert\\\\Cache\\\\chexbert.pth'\n",
      "Predictions test:\n",
      "\n",
      "TEST GEN 1: 1. interval appearance of endotracheal tube which is 4 cm above the carina. a defibrillator pad overlies the left hemithorax. 2. right lower lobe opacification with a small to moderate right pl\n",
      "TEST TARGET 1: 1. stable right internal jugular central venous catheter, prosthetic heart valve, and median sternotomy wires. 2. loculated right pleural effusion, low lung volumes in the right hemithorax, and mild cardiomegaly are unchanged. 3. slight increase in diffuse interstitial opacities, likely reflecting underlying mild pulmonary edema increased.\n",
      "[Info] bert-score not installed/available; BERTScore will be NaN. `pip install bert-score`.\n",
      "BLEU: 0.0056 | ROUGE-L: 0.1026 | METEOR: 0.1002 | BERTScore(F1): nan\n",
      "\n",
      "TEST GEN 2: 1. supine frontal view of the chest demonstrates interval removal of the enteric tube. the remaining right ij catheter and surgical materials are stable. 2. the heart is moderately enlarged and mitral annular calcification as well as\n",
      "TEST TARGET 2: 1. stable right internal jugular central venous catheter with tip in the superior vena cava. mitral and tricuspid valves are again seen. median sternotomy wires. nasogastric tube extends into the stomach. 2. loculated right pleural effusion is unchanged. 3. prominent interstitial lung markings bilaterally likely related to pulmonary edema versus infection unchanged. 4. cardiomediastinal silhouette stable in size and appearance.\n",
      "BLEU: 0.0049 | ROUGE-L: 0.1649 | METEOR: 0.1250 | BERTScore(F1): nan\n",
      "\n",
      "TEST GEN 3: 1. increased left lower lung zone opacity. 2. blunting of the right costophrenic angle with a small right pleural effusion. 3. stable mitral valve replacement and annular ring in the tricuspid\n",
      "TEST TARGET 3: 1. findings consistent with removal of a significant amount of right pleural fluid. no pneumothorax. 2. stable small left pleural effusion with retrocardiac opacity which may represent atelectasis or consolidation.\n",
      "BLEU: 0.0191 | ROUGE-L: 0.1967 | METEOR: 0.3180 | BERTScore(F1): nan\n",
      "\n",
      "TEST GEN 4: 1. tracheostomy tube remains in place. right picc is in stable and standard position. the enteric tube has been removed. 2. no evidence of pneumothorax. slightly improved aeration of the bilateral lungs with\n",
      "TEST TARGET 4: stable bilateral pleural effusions and bibasilar consolidation.\n",
      "BLEU: 0.0074 | ROUGE-L: 0.1000 | METEOR: 0.1802 | BERTScore(F1): nan\n",
      "\n",
      "TEST GEN 5: 1. stable post - surgical thorax with median sternotomy wires and surgical clips overlying the left hilum. dense calcifications are seen projecting over the right hilum. these may be related to underlying calcified lymph\n",
      "TEST TARGET 5: 1. compared to prior examination, there is significant interval decrease to near resolution of the left pleural effusion. the right pleural effusion and elevated right hemidiaphragm is grossly unchanged. stable postsurgical changes.\n",
      "BLEU: 0.0195 | ROUGE-L: 0.1538 | METEOR: 0.1856 | BERTScore(F1): nan\n",
      "\n",
      "TEST GEN 6: 1. supine frontal view of the chest demonstrates interval removal of the enteric tube. the remaining right ij catheter and surgical materials are stable. 2. the heart is moderately enlarged and mitral annular calcification as well as\n",
      "TEST TARGET 6: 1. semiupright frontal view of the chest demonstrates a stable right - sided picc line. 2. there is moderate pulmonary edema with a left retrocardiac opacity. 3. right middle and lower lobe opacities may represent aspiration versus infection. 4. a right loculated pleural effusion continues.\n",
      "BLEU: 0.1151 | ROUGE-L: 0.2963 | METEOR: 0.2573 | BERTScore(F1): nan\n",
      "\n",
      "TEST GEN 7: 1. supine frontal view of the chest demonstrates interval removal of the enteric tube. the remaining right ij catheter and surgical materials are stable. 2. the heart is moderately enlarged and mitral annular calcification as well as\n",
      "TEST TARGET 7: 1. stable chest radiograph, with small right sided pleural effusion, blunting of the left costophrenic angle which could represent scarring or pleural thickening. otherwise clear lungs.\n",
      "BLEU: 0.0133 | ROUGE-L: 0.1270 | METEOR: 0.1406 | BERTScore(F1): nan\n",
      "\n",
      "TEST GEN 8: 1. frontal and lateral views of the chest demonstrate interval removal of the right central venous catheter. no evidence for pneumothorax. 2. lateral view is limited due to the patient ' s inability to raise his arm.\n",
      "TEST TARGET 8: 1. compared to prior examination, there is significant interval decrease to near resolution of the left pleural effusion. the right pleural effusion and elevated right hemidiaphragm is grossly unchanged. stable postsurgical changes.\n",
      "BLEU: 0.0188 | ROUGE-L: 0.1765 | METEOR: 0.2239 | BERTScore(F1): nan\n",
      "\n",
      "--- Batch means (text metrics) ---\n",
      "BLEU: 0.0255 | ROUGE-L: 0.1647 | METEOR: 0.1913 | BERTScore(F1): nan\n"
     ]
    }
   ],
   "source": [
    "# Installs you might need:\n",
    "# !pip install -U nltk rouge-score bert-score f1chexbert requests appdirs\n",
    "\n",
    "import os, shutil, requests\n",
    "import torch, numpy as np\n",
    "from appdirs import user_cache_dir\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ---- Optional imports (guarded) ----\n",
    "try:\n",
    "    from bert_score import score as bertscore_score\n",
    "except Exception:\n",
    "    bertscore_score = None\n",
    "\n",
    "# ---- CheXbert cache utils ----\n",
    "CHEXBERT_URLS = [\n",
    "    # Official Stanford Box link (may redirect / require retries)\n",
    "    \"https://stanfordmedicine.box.com/shared/static/c3stck6w6dol3h36grdc97xoydzxd7w9\",\n",
    "    # Hugging Face mirror (often more reliable)\n",
    "    \"https://huggingface.co/StanfordAIMI/RRG_scorers/resolve/main/chexbert.pth\",\n",
    "]\n",
    "\n",
    "def default_chexbert_cache_dir():\n",
    "    # Matches f1chexbert's default on Windows:\n",
    "    # C:\\Users\\<YOU>\\AppData\\Local\\chexbert\\chexbert\\Cache\n",
    "    return os.path.join(user_cache_dir(\"chexbert\", \"chexbert\"), \"Cache\")\n",
    "\n",
    "def chexbert_ckpt_path(cache_dir=None, filename=\"chexbert.pth\"):\n",
    "    cache_dir = cache_dir or default_chexbert_cache_dir()\n",
    "    return os.path.join(cache_dir, filename)\n",
    "\n",
    "def delete_chexbert_weights(cache_dir=None, filename=\"chexbert.pth\"):\n",
    "    \"\"\"Delete existing CheXbert weights if present.\"\"\"\n",
    "    path = chexbert_ckpt_path(cache_dir, filename)\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            os.remove(path)\n",
    "            print(f\"[CheXbert] Deleted old weights at: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[CheXbert] Could not delete {path}: {e}\")\n",
    "    else:\n",
    "        print(\"[CheXbert] No existing weights to delete.\")\n",
    "\n",
    "def ensure_chexbert_weights(cache_dir=None, filename=\"chexbert.pth\", force=False, min_bytes=10_000_000):\n",
    "    \"\"\"Ensure weights exist. If force=True, redownload even if file exists.\"\"\"\n",
    "    cache_dir = cache_dir or default_chexbert_cache_dir()\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    dst = os.path.join(cache_dir, filename)\n",
    "\n",
    "    if not force and os.path.exists(dst) and os.path.getsize(dst) > min_bytes:\n",
    "        return dst  # looks valid\n",
    "\n",
    "    # (Re)download\n",
    "    for url in CHEXBERT_URLS:\n",
    "        try:\n",
    "            print(f\"[CheXbert] Downloading weights from: {url}\")\n",
    "            with requests.get(url, stream=True, timeout=120, allow_redirects=True) as r:\n",
    "                r.raise_for_status()\n",
    "                tmp = dst + \".tmp\"\n",
    "                with open(tmp, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                os.replace(tmp, dst)\n",
    "            if os.path.getsize(dst) > min_bytes:\n",
    "                print(f\"[CheXbert] Weights saved to: {dst}\")\n",
    "                return dst\n",
    "            else:\n",
    "                print(f\"[CheXbert] Downloaded file too small ({os.path.getsize(dst)} bytes), trying next mirror...\")\n",
    "        except Exception as e:\n",
    "            print(f\"[CheXbert] Download failed from {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "# ---- Delete then (re)download weights ----\n",
    "delete_chexbert_weights()\n",
    "_ckpt_path = ensure_chexbert_weights(force=True)\n",
    "\n",
    "# ---- Try to load CheXbert evaluator ----\n",
    "try:\n",
    "    from f1chexbert import F1CheXbert\n",
    "    _chexbert = F1CheXbert() if _ckpt_path else None\n",
    "    if _chexbert is None:\n",
    "        print(\"[Info] CheXbert unavailable (no weights). Place chexbert.pth in the cache dir above.\")\n",
    "except Exception as e:\n",
    "    _chexbert = None\n",
    "    print(f\"[Info] CheXbert not available: {e}\")\n",
    "\n",
    "# ---- Helpers ----\n",
    "def tok(s: str):\n",
    "    return [w for w in wordpunct_tokenize(s.lower()) if w.strip()]\n",
    "\n",
    "_warned_bert = False\n",
    "def safe_bertscore(pred: str, tgt: str) -> float:\n",
    "    global _warned_bert\n",
    "    if bertscore_score is None:\n",
    "        if not _warned_bert:\n",
    "            print(\"[Info] bert-score not installed/available; BERTScore will be NaN. `pip install bert-score`.\")\n",
    "            _warned_bert = True\n",
    "        return float(\"nan\")\n",
    "    P, R, F = bertscore_score([pred], [tgt], lang=\"en\", rescale_with_baseline=True)\n",
    "    return float(F.mean().item())\n",
    "\n",
    "# ---- Metric setup ----\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "bleu_list, rougeL_list, meteor_list, bert_list = [], [], [], []\n",
    "gens_all, tgts_all = [], []\n",
    "\n",
    "# ===== Evaluation loop (single batch), assumes: valid_loader, model, tokenizer, device, bos_id, eos_id =====\n",
    "test_loader_sliced = iter(valid_loader)\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_sliced:\n",
    "        pixel_values, ids_loader, paths, raw_labels = batch\n",
    "        pixel_values = pixel_values.to(device)\n",
    "\n",
    "        gen_ids = model.generate(\n",
    "            pixel_values=pixel_values,\n",
    "            bos_id=bos_id, eos_id=eos_id,\n",
    "            max_new_tokens=50, top_p=0.8, temperature=0.9, greedy=False\n",
    "        )\n",
    "\n",
    "        print(\"Predictions test:\")\n",
    "        for i in range(gen_ids.size(0)):\n",
    "            # Skip special tokens to avoid artifacts\n",
    "            gen_text = tokenizer.decode(gen_ids[i].tolist())\n",
    "            tgt_text = tokenizer.decode(ids_loader[i].tolist())\n",
    "\n",
    "            print(f\"\\nTEST GEN {i+1}:\", gen_text)\n",
    "            print(f\"TEST TARGET {i+1}:\", tgt_text)\n",
    "\n",
    "            # BLEU\n",
    "            bleu = sentence_bleu([tgt_text.split()], gen_text.split(), smoothing_function=smooth)\n",
    "            # ROUGE-L (F)\n",
    "            rougeL = scorer.score(tgt_text, gen_text)['rougeL'].fmeasure\n",
    "            # METEOR (token lists)\n",
    "            try:\n",
    "                meteor = meteor_score([tok(tgt_text)], tok(gen_text))\n",
    "            except TypeError:\n",
    "                meteor = meteor_score([tgt_text], gen_text)\n",
    "            # BERTScore\n",
    "            bert = safe_bertscore(gen_text, tgt_text)\n",
    "\n",
    "            print(f\"BLEU: {bleu:.4f} | ROUGE-L: {rougeL:.4f} | METEOR: {meteor:.4f} | BERTScore(F1): {bert:.4f}\")\n",
    "\n",
    "            bleu_list.append(bleu); rougeL_list.append(rougeL); meteor_list.append(meteor); bert_list.append(bert)\n",
    "            gens_all.append(gen_text); tgts_all.append(tgt_text)\n",
    "\n",
    "        def nanmean(x):\n",
    "            arr = np.array(x, dtype=float)\n",
    "            return float(np.nanmean(arr)) if arr.size else float(\"nan\")\n",
    "\n",
    "        print(\"\\n--- Batch means (text metrics) ---\")\n",
    "        print(f\"BLEU: {nanmean(bleu_list):.4f} | ROUGE-L: {nanmean(rougeL_list):.4f} | \"\n",
    "              f\"METEOR: {nanmean(meteor_list):.4f} | BERTScore(F1): {nanmean(bert_list):.4f}\")\n",
    "\n",
    "        # ---- CheXbert (batch clinical metric) ----\n",
    "        if _chexbert is not None and gens_all and len(gens_all) == len(tgts_all):\n",
    "            try:\n",
    "                chex_accuracy, chex_acc_not_avg, chex_report, chex_report_5 = _chexbert(\n",
    "                    hyps=gens_all, refs=tgts_all\n",
    "                )\n",
    "                print(\"\\n--- CheXbert (batch) ---\")\n",
    "                print(f\"CheXbert macro F1 (called 'accuracy' in pkg): {float(chex_accuracy):.4f}\")\n",
    "                if isinstance(chex_report_5, dict):\n",
    "                    top5 = ', '.join([f\"{k}: {v:.3f}\" for k, v in chex_report_5.items()])\n",
    "                    print(f\"Top-5 precision [Cardiomegaly, Edema, Consolidation, Atelectasis, Pleural Effusion]: {top5}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Warn] CheXbert failed to run: {e}\")\n",
    "\n",
    "        # Cleanup\n",
    "        del pixel_values, ids_loader, paths, raw_labels, gen_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a02a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DINOv3PEF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
