{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd81e504",
   "metadata": {},
   "source": [
    "Model creation and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d90fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded segmenter weights from models/dino_unet_decoder_finetuned.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from utils.models.complete_model import create_complete_model, save_complete_model, load_complete_model, save_checkpoint, load_checkpoint\n",
    "\n",
    "\n",
    "# ALL PATHS\n",
    "MODELS_DIR = \"models/\"\n",
    "SEGMENTER_MODEL_PATH = f\"{MODELS_DIR}dino_unet_decoder_finetuned.pth\"\n",
    "save_path = f\"{MODELS_DIR}complete_model.pth\"\n",
    "checkpoint_path = f\"{MODELS_DIR}model_checkpoint.pth\"\n",
    "\n",
    "# Example usage\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = create_complete_model(device=device, SEGMENTER_MODEL_PATH=SEGMENTER_MODEL_PATH)\n",
    "\n",
    "# Load the model\n",
    "if Path(save_path).exists():\n",
    "    model = load_complete_model(model, save_path, device=device, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97d42a",
   "metadata": {},
   "source": [
    "Data loader creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db9bcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local images_dir detected; filtering rows with missing PNGs...\n",
      "[INFO] Kept 52/202 rows with existing PNGs\n",
      "Local images_dir detected; filtering rows with missing PNGs...\n",
      "[INFO] Kept 52/202 rows with existing PNGs\n",
      "Batch image tensor shape: torch.Size([4, 3, 512, 512])\n",
      "Batch findings shape: 4\n",
      "Batch image paths shape: 4\n"
     ]
    }
   ],
   "source": [
    "from utils.data.dataloaders import create_dataloaders\n",
    "\n",
    "# CheXpert\n",
    "CHEXPERT_DIR = \"Datasets/CheXpertPlus\"\n",
    "chexpert_paths = {\n",
    "    \"chexpert_data_path\": f\"{CHEXPERT_DIR}/PNG\",  # base PNG folder\n",
    "    \"chexpert_data_csv\": f\"{CHEXPERT_DIR}/df_chexpert_plus_240401.csv\",\n",
    "}\n",
    "\n",
    "# MIMIC\n",
    "MIMIC_DIR = \"Datasets/MIMIC\"\n",
    "mimic_paths = {\n",
    "    \"mimic_data_path\": MIMIC_DIR,\n",
    "    \"mimic_splits_csv\": f\"{MIMIC_DIR}/mimic-cxr-2.0.0-split.csv.gz\",\n",
    "    \"mimic_metadata_csv\": f\"{MIMIC_DIR}/mimic-cxr-2.0.0-metadata.csv\",\n",
    "    \"mimic_reports_path\": f\"{MIMIC_DIR}/cxr-record-list.csv.gz\",  # must contain 'path'\n",
    "    \"mimic_images_dir\": f\"{MIMIC_DIR}/matched_images_and_masks_mimic_224/images\",\n",
    "}\n",
    "\n",
    "import os\n",
    "kwargs = {\n",
    "    # \"num_workers\": os.cpu_count() // 2 if os.cpu_count() else 4,  # adjust on your VM\n",
    "    # \"persistent_workers\": True,           # reuses workers between iterations\n",
    "    # \"prefetch_factor\": 4,                 # each worker prefetches batches\n",
    "    # \"pin_memory\": True,                   # if using CUDA\n",
    "    # \"drop_last\": False\n",
    "}\n",
    "\n",
    "train_loader = create_dataloaders(\n",
    "    chexpert_paths, \n",
    "    mimic_paths, \n",
    "    batch_size=4,\n",
    "    split=\"test\", \n",
    "    sampling_ratio=0.7,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "valid_loader = create_dataloaders(\n",
    "    chexpert_paths,\n",
    "    mimic_paths,\n",
    "    batch_size=4,\n",
    "    split=\"test\",\n",
    "    sampling_ratio=0.7,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "images, findings, image_paths, _ = next(iter(train_loader))\n",
    "print(\"Batch image tensor shape:\", getattr(images, \"shape\", \"N/A\"))\n",
    "print(\"Batch findings shape:\", getattr(findings, \"shape\", len(findings)))\n",
    "print(\"Batch image paths shape:\", getattr(image_paths, \"shape\", len(image_paths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab93af2",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loop.py (resumable + seeded best_metric + cumulative tokens_cum)\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional, Literal, Dict, Any, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Checkpoint utilities (enhanced + global_step + tokens_cum)\n",
    "# ============================================================\n",
    "def save_checkpoint(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    save_path: str,\n",
    "    *,\n",
    "    epoch: int | None = None,\n",
    "    global_step: int | None = None,\n",
    "    tokens_cum: int | None = None,           # <- NEW: persist cumulative tokens\n",
    "    best_metric: float | None = None,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler | None = None,\n",
    "    scaler: torch.cuda.amp.GradScaler | None = None,\n",
    "    extra: dict | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Saves model/optimizer states with your original key names plus metadata.\n",
    "    Keeps: 'model_state_dict', 'optimizer_state_dict'\n",
    "    Adds: epoch/global_step/tokens_cum/best_metric/scheduler/scaler/extra\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"tokens_cum\": tokens_cum,  # <- NEW\n",
    "        \"best_metric\": best_metric,  # ES-internal scale (loss for \"min\", negated metric for \"max\")\n",
    "        \"scheduler_state_dict\": scheduler.state_dict() if scheduler is not None else None,\n",
    "        \"scaler_state_dict\": (scaler.state_dict() if (scaler is not None and scaler.is_enabled()) else None),\n",
    "        \"checkpoint_version\": 3,\n",
    "    }\n",
    "    if extra:\n",
    "        checkpoint[\"extra\"] = extra\n",
    "\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Saved checkpoint to {save_path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    load_path: str,\n",
    "    device: str = \"cpu\",\n",
    "    *,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler | None = None,\n",
    "    scaler: torch.cuda.amp.GradScaler | None = None,\n",
    "    strict: bool = False,\n",
    ") -> tuple[nn.Module, torch.optim.Optimizer, dict]:\n",
    "    \"\"\"\n",
    "    Loads weights/optimizer and, if present, scheduler/scaler metadata.\n",
    "    Returns (model, optimizer, meta_dict) with 'epoch', 'global_step', 'tokens_cum' if present.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(load_path):\n",
    "        print(f\"No checkpoint found at {load_path}\")\n",
    "        model.to(device)\n",
    "        return model, optimizer, {}\n",
    "\n",
    "    ckpt = torch.load(load_path, map_location=\"cpu\")\n",
    "\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"], strict=strict)\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "\n",
    "    if scheduler is not None and ckpt.get(\"scheduler_state_dict\") is not None:\n",
    "        try:\n",
    "            scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not load scheduler state: {e}\")\n",
    "\n",
    "    if scaler is not None and ckpt.get(\"scaler_state_dict\") is not None:\n",
    "        try:\n",
    "            scaler.load_state_dict(ckpt[\"scaler_state_dict\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not load AMP scaler state: {e}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    meta = {\n",
    "        \"epoch\": ckpt.get(\"epoch\"),\n",
    "        \"global_step\": ckpt.get(\"global_step\", 0),\n",
    "        \"tokens_cum\": ckpt.get(\"tokens_cum\", 0),  # <- NEW\n",
    "        \"best_metric\": ckpt.get(\"best_metric\"),\n",
    "        \"extra\": ckpt.get(\"extra\", {}),\n",
    "        \"checkpoint_version\": ckpt.get(\"checkpoint_version\", 0),\n",
    "    }\n",
    "    print(\n",
    "        f\"Loaded checkpoint from {load_path} \"\n",
    "        f\"(epoch={meta['epoch']}, global_step={meta['global_step']}, tokens_cum={meta['tokens_cum']}, best_metric={meta['best_metric']})\"\n",
    "    )\n",
    "    return model, optimizer, meta\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Early Stopping machinery\n",
    "# =======================\n",
    "@dataclass\n",
    "class EarlyStoppingConfig:\n",
    "    patience: int = 5\n",
    "    min_delta: float = 0.0\n",
    "    mode: Literal[\"min\", \"max\"] = \"min\"\n",
    "    restore_best: bool = True\n",
    "    best_ckpt_path: str = \"model_checkpoint_best.pth\"\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, cfg: EarlyStoppingConfig):\n",
    "        self.cfg = cfg\n",
    "        self.best = float(\"inf\") if cfg.mode == \"min\" else -float(\"inf\")\n",
    "        self.num_bad_epochs = 0\n",
    "        self.should_stop = False\n",
    "\n",
    "    def _is_better(self, metric: float) -> bool:\n",
    "        if self.cfg.mode == \"min\":\n",
    "            return (self.best - metric) > self.cfg.min_delta\n",
    "        else:\n",
    "            return (metric - self.best) > self.cfg.min_delta\n",
    "\n",
    "    def step(self, metric: float) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if improved (and resets patience), else False.\n",
    "        \"\"\"\n",
    "        if self._is_better(metric):\n",
    "            self.best = metric\n",
    "            self.num_bad_epochs = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "            if self.num_bad_epochs >= self.cfg.patience:\n",
    "                self.should_stop = True\n",
    "            return False\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utility helpers for logging\n",
    "# =========================\n",
    "def _count_nonpad_tokens(tgt_ids: torch.Tensor, pad_token_id: int) -> int:\n",
    "    return int((tgt_ids != pad_token_id).sum().item())\n",
    "\n",
    "\n",
    "def _get_lr(optimizer: torch.optim.Optimizer) -> float:\n",
    "    return float(optimizer.param_groups[0].get(\"lr\", 0.0))\n",
    "\n",
    "\n",
    "# ===============\n",
    "# Main train loop\n",
    "# ===============\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: Iterable,\n",
    "    valid_loader: Optional[Iterable],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epochs: int = 10,\n",
    "    device: str = \"cpu\",\n",
    "    log_dir: str = \"runs/exp1\",\n",
    "    checkpoint_path: str = \"model_checkpoint.pth\",\n",
    "    is_on_cloud: bool = False,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    validate_every: int = 1,\n",
    "    ckpt_every: int = 5,\n",
    "    use_amp: Optional[bool] = None,\n",
    "    grad_accum_steps: int = 1,\n",
    "    # Scheduler\n",
    "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler | torch.optim.lr_scheduler.ReduceLROnPlateau] = None,\n",
    "    scheduler_step_on: Literal[\"epoch\", \"step\", \"val_metric\"] = \"epoch\",\n",
    "    # Early stopping\n",
    "    early_stopping: Optional[EarlyStopping] = None,\n",
    "    # ---- Resume controls ----\n",
    "    resume_from: str | None = None,      # path to .pth to resume from\n",
    "    start_epoch: int | None = None,      # overrides checkpoint epoch if provided\n",
    "    start_global_step: int | None = None, # overrides checkpoint global_step if provided\n",
    "    start_tokens_cum: int | None = None,  # overrides checkpoint tokens_cum if provided\n",
    "):\n",
    "    \"\"\"\n",
    "    Full training loop with AMP, grad accumulation, LR scheduler, early stopping, and TensorBoard logging.\n",
    "    Supports resuming from checkpoint with continued TensorBoard steps and cumulative tokens.\n",
    "\n",
    "    Expected batch format:\n",
    "      (images: FloatTensor[B,C,H,W], findings: list[str], image_path, report_path)\n",
    "\n",
    "    Model interface:\n",
    "      model.forward(pixel_values, tgt_ids) -> object with `.loss` when labels are provided.\n",
    "      model.tokenizer must provide `pad_token_id`.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # AMP setup\n",
    "    if use_amp is None:\n",
    "        use_amp = (device.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(device=device, enabled=use_amp)\n",
    "    autocast = torch.amp.autocast(device.type, enabled=use_amp)\n",
    "\n",
    "    # --------- RESUME STATE ---------\n",
    "    _epoch0 = 0\n",
    "    global_step = 0\n",
    "    tokens_cum = 0  # <- NEW: global cumulative tokens that never reset\n",
    "    best_metric = float(\"inf\") if (early_stopping and early_stopping.cfg.mode == \"min\") else -float(\"inf\")\n",
    "\n",
    "    if resume_from is not None and os.path.exists(resume_from):\n",
    "        print(f\"ðŸ” Resuming from checkpoint: {resume_from}\")\n",
    "        _, _, meta = load_checkpoint(\n",
    "            model,\n",
    "            optimizer,\n",
    "            resume_from,\n",
    "            device=str(device),\n",
    "            scheduler=scheduler,\n",
    "            scaler=scaler,\n",
    "            strict=False,\n",
    "        )\n",
    "        _epoch0 = int(meta.get(\"epoch\") or 0)\n",
    "        global_step = int(meta.get(\"global_step\") or 0)\n",
    "        tokens_cum = int(meta.get(\"tokens_cum\") or 0)  # <- NEW\n",
    "        if meta.get(\"best_metric\") is not None:\n",
    "            best_metric = meta[\"best_metric\"]\n",
    "            if early_stopping is not None:\n",
    "                early_stopping.best = best_metric  # âœ… Seed ES best for correct comparisons\n",
    "\n",
    "    # Optional manual overrides\n",
    "    if start_epoch is not None:\n",
    "        _epoch0 = int(start_epoch)\n",
    "    if start_global_step is not None:\n",
    "        global_step = int(start_global_step)\n",
    "    if start_tokens_cum is not None:\n",
    "        tokens_cum = int(start_tokens_cum)\n",
    "\n",
    "    # TensorBoard: same log_dir â†’ new events append; steps continue from global_step\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    print(\n",
    "        f\"TensorBoard logging to: {log_dir} \"\n",
    "        f\"(starting global_step={global_step}, tokens_cum={tokens_cum}, start_epoch={_epoch0+1})\"\n",
    "    )\n",
    "\n",
    "    best_epoch = _epoch0\n",
    "\n",
    "    # ------------------ EPOCHS ------------------\n",
    "    for epoch in range(_epoch0 + 1, epochs + 1):\n",
    "        if is_on_cloud:\n",
    "            print(\"[CLOUD CHECK] Ensure credits > $10 USD before continuingâ€¦\")\n",
    "\n",
    "        # ----------------- TRAIN -----------------\n",
    "        model.train()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        epoch_start = time.time()\n",
    "        running_loss = 0.0\n",
    "        step = 0\n",
    "        tokens_this_epoch = 0\n",
    "        wall_start = time.time()\n",
    "\n",
    "        print(f\"\\nðŸŸ¡ Epoch {epoch}/{epochs} â€” training\")\n",
    "        for batch in tqdm(train_loader):\n",
    "            if step > 10:\n",
    "                break\n",
    "            images, findings, *_ = batch\n",
    "\n",
    "            # Tokenize target strings\n",
    "            tok = model.tokenizer(\n",
    "                findings,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tgt_ids = tok[\"input_ids\"].to(device, non_blocking=True)\n",
    "            images = images.to(device, non_blocking=True)\n",
    "\n",
    "            with autocast:\n",
    "                out = model(pixel_values=images, tgt_ids=tgt_ids)\n",
    "                if not hasattr(out, \"loss\") or out.loss is None:\n",
    "                    raise RuntimeError(\"Model forward did not return .loss. Ensure labels are built inside model.forward.\")\n",
    "                loss = out.loss / grad_accum_steps\n",
    "\n",
    "            # Backward\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            # Update counters\n",
    "            step += 1\n",
    "            global_step += 1\n",
    "            running_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            ntoks = _count_nonpad_tokens(tgt_ids, model.pad_token_id)\n",
    "            tokens_this_epoch += ntoks\n",
    "            tokens_cum += ntoks  # <- NEW: global cumulative count never resets\n",
    "\n",
    "            # Optimizer step on accumulation boundary\n",
    "            if step % grad_accum_steps == 0:\n",
    "                if use_amp:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                total_norm = clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                if use_amp:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                # Scheduler stepping per-step\n",
    "                if scheduler is not None and scheduler_step_on == \"step\":\n",
    "                    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        scheduler.step(running_loss / max(1, step))\n",
    "                    else:\n",
    "                        scheduler.step()\n",
    "\n",
    "                # TB per-step\n",
    "                writer.add_scalar(\"train/loss_step\", running_loss / max(1, step), global_step)\n",
    "                writer.add_scalar(\"train/lr\", _get_lr(optimizer), global_step)\n",
    "                writer.add_scalar(\"train/grad_norm\", float(total_norm), global_step)\n",
    "                writer.add_scalar(\"train/tokens_cum\", tokens_cum, global_step)  # <- NEW: log global cumulative\n",
    "\n",
    "        # End epoch: aggregate train stats\n",
    "        train_time = time.time() - epoch_start\n",
    "        avg_train_loss = running_loss / max(1, step)\n",
    "        train_ppl = math.exp(avg_train_loss) if avg_train_loss < 20 else float(\"inf\")\n",
    "        toks_per_s = tokens_this_epoch / max(1e-6, (time.time() - wall_start))\n",
    "\n",
    "        print(\n",
    "            f\"âœ… Train â€” loss: {avg_train_loss:.4f} | pplâ‰ˆ {train_ppl:.2f} | \"\n",
    "            f\"steps: {step} | tokens_epoch: {tokens_this_epoch} | tokens_cum: {tokens_cum} | \"\n",
    "            f\"time: {train_time:.1f}s | toks/sâ‰ˆ {toks_per_s:.1f}\"\n",
    "        )\n",
    "        writer.add_scalar(\"train/loss_epoch\", avg_train_loss, epoch)\n",
    "        writer.add_scalar(\"train/ppl_epoch\", train_ppl if math.isfinite(train_ppl) else 0.0, epoch)\n",
    "        writer.add_scalar(\"train/tokens_epoch\", tokens_this_epoch, epoch)\n",
    "        writer.add_scalar(\"train/tokens_cum_epoch_close\", tokens_cum, epoch)  # epoch-close snapshot of global tokens\n",
    "        writer.add_scalar(\"time/train_epoch_s\", train_time, epoch)\n",
    "\n",
    "        # Scheduler per-epoch (non-plateau)\n",
    "        if scheduler is not None and scheduler_step_on == \"epoch\" and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step()\n",
    "\n",
    "        # ----------------- VALID -----------------\n",
    "        do_validate = (valid_loader is not None) and (epoch % validate_every == 0)\n",
    "        avg_val_loss = None\n",
    "        if do_validate:\n",
    "            model.eval()\n",
    "            val_loss_sum = 0.0\n",
    "            val_steps = 0\n",
    "            val_tokens = 0\n",
    "\n",
    "            print(f\"ðŸ”µ Epoch {epoch}/{epochs} â€” validating\")\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(valid_loader):\n",
    "                    if val_steps > 10:\n",
    "                        break\n",
    "                    v_images, v_findings, *_ = batch\n",
    "\n",
    "                    tok = model.tokenizer(\n",
    "                        v_findings,\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    v_tgt_ids = tok[\"input_ids\"].to(device, non_blocking=True)\n",
    "                    v_images = v_images.to(device, non_blocking=True)\n",
    "\n",
    "                    with autocast:\n",
    "                        out = model(pixel_values=v_images, tgt_ids=v_tgt_ids)\n",
    "                        if not hasattr(out, \"loss\") or out.loss is None:\n",
    "                            raise RuntimeError(\"Model forward did not return .loss on validation.\")\n",
    "                        v_loss = out.loss\n",
    "\n",
    "                    val_loss_sum += float(v_loss.item())\n",
    "                    val_steps += 1\n",
    "                    val_tokens += _count_nonpad_tokens(v_tgt_ids, model.pad_token_id)\n",
    "\n",
    "            avg_val_loss = val_loss_sum / max(1, val_steps)\n",
    "            val_ppl = math.exp(avg_val_loss) if avg_val_loss < 20 else float(\"inf\")\n",
    "            print(f\"ðŸ“˜ Valid â€” loss: {avg_val_loss:.4f} | pplâ‰ˆ {val_ppl:.2f} | steps: {val_steps} | tokens_valid: {val_tokens}\")\n",
    "\n",
    "            writer.add_scalar(\"valid/loss_epoch\", avg_val_loss, epoch)\n",
    "            writer.add_scalar(\"valid/ppl_epoch\", val_ppl if math.isfinite(val_ppl) else 0.0, epoch)\n",
    "            writer.add_scalar(\"valid/tokens_epoch\", val_tokens, epoch)\n",
    "\n",
    "            # Scheduler on validation metric (ReduceLROnPlateau)\n",
    "            if scheduler is not None and scheduler_step_on == \"val_metric\" and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(avg_val_loss)\n",
    "\n",
    "            # Early stopping & best checkpoint\n",
    "            if early_stopping is not None:\n",
    "                # ES metric: for \"min\" we use avg_val_loss; for \"max\" you could pass a positive metric.\n",
    "                metric_for_es = avg_val_loss if early_stopping.cfg.mode == \"min\" else -avg_val_loss\n",
    "                improved = early_stopping.step(metric_for_es)\n",
    "\n",
    "                if improved:\n",
    "                    best_metric = metric_for_es\n",
    "                    best_epoch = epoch\n",
    "                    try:\n",
    "                        print(\"ðŸ’¾ New best model â€” saving best checkpointâ€¦\")\n",
    "                        save_checkpoint(\n",
    "                            model,\n",
    "                            optimizer,\n",
    "                            early_stopping.cfg.best_ckpt_path,\n",
    "                            epoch=epoch,\n",
    "                            global_step=global_step,\n",
    "                            tokens_cum=tokens_cum,  # <- NEW\n",
    "                            best_metric=best_metric,  # internal ES scale\n",
    "                            scheduler=scheduler,\n",
    "                            scaler=scaler,\n",
    "                            extra={\n",
    "                                \"phase\": \"best\",\n",
    "                                \"best_epoch\": epoch,\n",
    "                                \"best_val_loss\": float(avg_val_loss),  # human-friendly raw loss\n",
    "                            },\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] Could not save best checkpoint: {e}\")\n",
    "\n",
    "                if early_stopping.should_stop:\n",
    "                    print(f\"â›³ Early stopping triggered at epoch {epoch}. Best epoch: {best_epoch}.\")\n",
    "                    if early_stopping.cfg.restore_best:\n",
    "                        try:\n",
    "                            print(\"ðŸ” Restoring best checkpoint weightsâ€¦\")\n",
    "                            load_checkpoint(\n",
    "                                model,\n",
    "                                optimizer,\n",
    "                                early_stopping.cfg.best_ckpt_path,\n",
    "                                device=str(device),\n",
    "                                scheduler=scheduler,\n",
    "                                scaler=scaler,\n",
    "                                strict=False,\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(f\"[WARN] Failed to restore best checkpoint: {e}\")\n",
    "                    writer.close()\n",
    "                    return  # graceful stop\n",
    "\n",
    "        # Periodic checkpoint\n",
    "        if ckpt_every and (epoch % ckpt_every == 0):\n",
    "            try:\n",
    "                print(\"ðŸ’¾ Periodic checkpoint â€” savingâ€¦\")\n",
    "                save_checkpoint(\n",
    "                    model,\n",
    "                    optimizer,\n",
    "                    checkpoint_path,\n",
    "                    epoch=epoch,\n",
    "                    global_step=global_step,\n",
    "                    tokens_cum=tokens_cum,  # <- NEW\n",
    "                    best_metric=best_metric,\n",
    "                    scheduler=scheduler,\n",
    "                    scaler=scaler,\n",
    "                    extra={\n",
    "                        \"train_loss\": avg_train_loss,\n",
    "                        \"val_loss\": avg_val_loss,\n",
    "                        \"lr\": _get_lr(optimizer),\n",
    "                    },\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not save periodic checkpoint: {e}\")\n",
    "\n",
    "    writer.close()\n",
    "    print(\"ðŸŽ‰ Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9ab9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logging to: runs/chestx_exp1 (starting global_step=0, tokens_cum=0, start_epoch=1)\n",
      "\n",
      "ðŸŸ¡ Epoch 1/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/864 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "  1%|â–         | 11/864 [00:25<32:35,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 6.4023 | pplâ‰ˆ 603.22 | steps: 11 | tokens_epoch: 3556 | tokens_cum: 3556 | time: 25.2s | toks/sâ‰ˆ 141.0\n",
      "ðŸ”µ Epoch 1/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:11<14:44,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 4.3263 | pplâ‰ˆ 75.66 | steps: 11 | tokens_valid: 3736\n",
      "ðŸ’¾ New best model â€” saving best checkpointâ€¦\n",
      "Saved checkpoint to checkpoints/model_best.pth\n",
      "\n",
      "ðŸŸ¡ Epoch 2/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:34<44:52,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 4.3914 | pplâ‰ˆ 80.75 | steps: 11 | tokens_epoch: 4026 | tokens_cum: 7582 | time: 34.7s | toks/sâ‰ˆ 115.9\n",
      "ðŸ”µ Epoch 2/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:05<06:37,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 4.1218 | pplâ‰ˆ 61.67 | steps: 11 | tokens_valid: 4154\n",
      "ðŸ’¾ New best model â€” saving best checkpointâ€¦\n",
      "Saved checkpoint to checkpoints/model_best.pth\n",
      "ðŸ’¾ Periodic checkpoint â€” savingâ€¦\n",
      "Saved checkpoint to checkpoints/model_epoch.pth\n",
      "\n",
      "ðŸŸ¡ Epoch 3/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:38<49:28,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 4.0074 | pplâ‰ˆ 55.00 | steps: 11 | tokens_epoch: 3795 | tokens_cum: 11377 | time: 38.3s | toks/sâ‰ˆ 99.1\n",
      "ðŸ”µ Epoch 3/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:04<05:10,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 3.8795 | pplâ‰ˆ 48.40 | steps: 11 | tokens_valid: 3608\n",
      "ðŸ’¾ New best model â€” saving best checkpointâ€¦\n",
      "Saved checkpoint to checkpoints/model_best.pth\n",
      "\n",
      "ðŸŸ¡ Epoch 4/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:42<54:42,  3.85s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 3.9358 | pplâ‰ˆ 51.20 | steps: 11 | tokens_epoch: 3490 | tokens_cum: 14867 | time: 42.3s | toks/sâ‰ˆ 82.4\n",
      "ðŸ”µ Epoch 4/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:03<04:30,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 3.6377 | pplâ‰ˆ 38.00 | steps: 11 | tokens_valid: 4274\n",
      "ðŸ’¾ New best model â€” saving best checkpointâ€¦\n",
      "Saved checkpoint to checkpoints/model_best.pth\n",
      "ðŸ’¾ Periodic checkpoint â€” savingâ€¦\n",
      "Saved checkpoint to checkpoints/model_epoch.pth\n",
      "\n",
      "ðŸŸ¡ Epoch 5/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:47<1:01:52,  4.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 3.7971 | pplâ‰ˆ 44.57 | steps: 11 | tokens_epoch: 4134 | tokens_cum: 19001 | time: 47.9s | toks/sâ‰ˆ 86.3\n",
      "ðŸ”µ Epoch 5/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:09<12:22,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 3.6616 | pplâ‰ˆ 38.92 | steps: 11 | tokens_valid: 4807\n",
      "\n",
      "ðŸŸ¡ Epoch 6/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [01:05<1:24:03,  5.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 3.8102 | pplâ‰ˆ 45.16 | steps: 11 | tokens_epoch: 4262 | tokens_cum: 23263 | time: 65.0s | toks/sâ‰ˆ 65.5\n",
      "ðŸ”µ Epoch 6/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:11<14:30,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 3.2792 | pplâ‰ˆ 26.56 | steps: 11 | tokens_valid: 4190\n",
      "ðŸ’¾ New best model â€” saving best checkpointâ€¦\n",
      "Saved checkpoint to checkpoints/model_best.pth\n",
      "ðŸ’¾ Periodic checkpoint â€” savingâ€¦\n",
      "Saved checkpoint to checkpoints/model_epoch.pth\n",
      "\n",
      "ðŸŸ¡ Epoch 7/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [01:07<1:27:14,  6.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 3.3057 | pplâ‰ˆ 27.27 | steps: 11 | tokens_epoch: 4530 | tokens_cum: 27793 | time: 67.5s | toks/sâ‰ˆ 67.1\n",
      "ðŸ”µ Epoch 7/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:08<10:33,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 3.2386 | pplâ‰ˆ 25.50 | steps: 11 | tokens_valid: 3651\n",
      "ðŸ’¾ New best model â€” saving best checkpointâ€¦\n",
      "Saved checkpoint to checkpoints/model_best.pth\n",
      "\n",
      "ðŸŸ¡ Epoch 8/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [01:08<1:28:16,  6.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 3.3434 | pplâ‰ˆ 28.31 | steps: 11 | tokens_epoch: 3943 | tokens_cum: 31736 | time: 68.3s | toks/sâ‰ˆ 57.7\n",
      "ðŸ”µ Epoch 8/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:08<11:15,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 3.2417 | pplâ‰ˆ 25.58 | steps: 11 | tokens_valid: 3952\n",
      "ðŸ’¾ Periodic checkpoint â€” savingâ€¦\n",
      "Saved checkpoint to checkpoints/model_epoch.pth\n",
      "\n",
      "ðŸŸ¡ Epoch 9/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [01:11<1:31:47,  6.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 3.4035 | pplâ‰ˆ 30.07 | steps: 11 | tokens_epoch: 3776 | tokens_cum: 35512 | time: 71.0s | toks/sâ‰ˆ 53.2\n",
      "ðŸ”µ Epoch 9/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:10<13:11,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 3.0958 | pplâ‰ˆ 22.11 | steps: 11 | tokens_valid: 3890\n",
      "ðŸ’¾ New best model â€” saving best checkpointâ€¦\n",
      "Saved checkpoint to checkpoints/model_best.pth\n",
      "\n",
      "ðŸŸ¡ Epoch 10/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [01:10<1:31:06,  6.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 3.4517 | pplâ‰ˆ 31.55 | steps: 11 | tokens_epoch: 3868 | tokens_cum: 39380 | time: 70.5s | toks/sâ‰ˆ 54.9\n",
      "ðŸ”µ Epoch 10/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:09<12:45,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 3.3025 | pplâ‰ˆ 27.18 | steps: 11 | tokens_valid: 3936\n",
      "ðŸ’¾ Periodic checkpoint â€” savingâ€¦\n",
      "Saved checkpoint to checkpoints/model_epoch.pth\n",
      "\n",
      "ðŸŸ¡ Epoch 11/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [01:09<1:30:06,  6.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 3.4455 | pplâ‰ˆ 31.36 | steps: 11 | tokens_epoch: 4042 | tokens_cum: 43422 | time: 69.7s | toks/sâ‰ˆ 58.0\n",
      "ðŸ”µ Epoch 11/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:11<14:57,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 3.3515 | pplâ‰ˆ 28.55 | steps: 11 | tokens_valid: 4049\n",
      "\n",
      "ðŸŸ¡ Epoch 12/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [01:12<1:33:11,  6.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 3.5329 | pplâ‰ˆ 34.22 | steps: 11 | tokens_epoch: 4702 | tokens_cum: 48124 | time: 72.1s | toks/sâ‰ˆ 65.2\n",
      "ðŸ”µ Epoch 12/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:14<18:07,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 3.4144 | pplâ‰ˆ 30.40 | steps: 11 | tokens_valid: 3876\n",
      "ðŸ’¾ Periodic checkpoint â€” savingâ€¦\n",
      "Saved checkpoint to checkpoints/model_epoch.pth\n",
      "\n",
      "ðŸŸ¡ Epoch 13/20 â€” training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [01:13<1:35:11,  6.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train â€” loss: 3.4443 | pplâ‰ˆ 31.32 | steps: 11 | tokens_epoch: 4017 | tokens_cum: 52141 | time: 73.7s | toks/sâ‰ˆ 54.5\n",
      "ðŸ”µ Epoch 13/20 â€” validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 11/864 [00:10<14:09,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Valid â€” loss: 3.1703 | pplâ‰ˆ 23.82 | steps: 11 | tokens_valid: 4009\n",
      "â›³ Early stopping triggered at epoch 13. Best epoch: 9.\n",
      "ðŸ” Restoring best checkpoint weightsâ€¦\n",
      "Loaded checkpoint from checkpoints/model_best.pth (epoch=9, global_step=99, tokens_cum=35512, best_metric=3.095811193639582)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "optimizer = Adam(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
    "\n",
    "early = EarlyStopping(EarlyStoppingConfig(\n",
    "    patience=4, min_delta=1e-4, mode=\"min\", restore_best=True,\n",
    "    best_ckpt_path=\"checkpoints/model_best.pth\"\n",
    "))\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=20,                       # total target; not \"remaining\"\n",
    "    device=device,\n",
    "    log_dir=\"runs/chestx_exp1\",       # SAME dir to keep appending\n",
    "    checkpoint_path=\"checkpoints/model_epoch.pth\",\n",
    "    validate_every=1,\n",
    "    ckpt_every=2,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_step_on=\"val_metric\",\n",
    "    early_stopping=early,\n",
    "    resume_from=\"checkpoints/model_epoch.pth\",  # or model_best.pth if you prefer to start from best weights\n",
    "    # start_epoch=...,                 # optional override\n",
    "    # start_global_step=...,           # optional override\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1927c",
   "metadata": {},
   "source": [
    "Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bddb3508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved complete model weights to models/complete_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "save_complete_model(model, save_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f199d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-chest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
