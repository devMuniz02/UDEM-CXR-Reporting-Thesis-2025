{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cfbe8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
      "        is_causal=False, scale=None, enable_gqa=False) -> Tensor:\n",
      "\n",
      "    Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed,\n",
      "    and applying dropout if a probability greater than 0.0 is specified. The optional scale argument can only be\n",
      "    specified as a keyword argument.\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        # Efficient implementation equivalent to the following:\n",
      "        def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
      "                is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n",
      "            L, S = query.size(-2), key.size(-2)\n",
      "            scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
      "            attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
      "            if is_causal:\n",
      "                assert attn_mask is None\n",
      "                temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
      "                attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
      "\n",
      "            if attn_mask is not None:\n",
      "                if attn_mask.dtype == torch.bool:\n",
      "                    attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
      "                else:\n",
      "                    attn_bias = attn_mask + attn_bias\n",
      "\n",
      "            if enable_gqa:\n",
      "                key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
      "                value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
      "\n",
      "            attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
      "            attn_weight += attn_bias\n",
      "            attn_weight = torch.softmax(attn_weight, dim=-1)\n",
      "            attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
      "            return attn_weight @ value\n",
      "\n",
      "    .. warning::\n",
      "        This function is beta and subject to change.\n",
      "\n",
      "    .. warning::\n",
      "        This function always applies dropout according to the specified ``dropout_p`` argument.\n",
      "        To disable dropout during evaluation, be sure to pass a value of ``0.0`` when the module\n",
      "        that makes the function call is not in training mode.\n",
      "\n",
      "        For example:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            class MyModel(nn.Module):\n",
      "                def __init__(self, p=0.5):\n",
      "                    super().__init__()\n",
      "                    self.p = p\n",
      "\n",
      "                def forward(self, ...):\n",
      "                    return F.scaled_dot_product_attention(...,\n",
      "                        dropout_p=(self.p if self.training else 0.0))\n",
      "\n",
      "    Note:\n",
      "\n",
      "        There are currently three supported implementations of scaled dot product attention:\n",
      "\n",
      "            - `FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning`_\n",
      "            - `Memory-Efficient Attention`_\n",
      "            - A PyTorch implementation defined in C++ matching the above formulation\n",
      "\n",
      "        The function may call optimized kernels for improved performance when using the CUDA backend.\n",
      "        For all other backends, the PyTorch implementation will be used.\n",
      "\n",
      "        All implementations are enabled by default. Scaled dot product attention attempts to automatically select the\n",
      "        most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation\n",
      "        is used, the following functions are provided for enabling and disabling implementations.\n",
      "        The context manager is the preferred mechanism:\n",
      "\n",
      "            - :func:`torch.nn.attention.sdpa_kernel`: A context manager used to enable or disable any of the implementations.\n",
      "            - :func:`torch.backends.cuda.enable_flash_sdp`: Globally enables or disables FlashAttention.\n",
      "            - :func:`torch.backends.cuda.enable_mem_efficient_sdp`: Globally enables or disables  Memory-Efficient Attention.\n",
      "            - :func:`torch.backends.cuda.enable_math_sdp`: Globally enables or disables  the PyTorch C++ implementation.\n",
      "\n",
      "        Each of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation,\n",
      "        disable the PyTorch C++ implementation using :func:`torch.nn.attention.sdpa_kernel`.\n",
      "        In the event that a fused implementation is not available, a warning will be raised with the\n",
      "        reasons why the fused implementation cannot run.\n",
      "\n",
      "        Due to the nature of fusing floating point operations, the output of this function may be different\n",
      "        depending on what backend kernel is chosen.\n",
      "        The c++ implementation supports torch.float64 and can be used when higher precision is required.\n",
      "        For math backend, all intermediates are kept in torch.float if inputs are in torch.half or torch.bfloat16.\n",
      "    For more information please see :doc:`/notes/numerical_accuracy`\n",
      "\n",
      "        Grouped Query Attention (GQA) is an experimental feature. It currently works only for Flash_attention\n",
      "        and math kernel on CUDA tensor, and does not support Nested tensor.\n",
      "        Constraints for GQA:\n",
      "\n",
      "            - number_of_heads_query % number_of_heads_key_value == 0 and,\n",
      "            - number_of_heads_key == number_of_heads_value\n",
      "\n",
      "    Note:\n",
      "\n",
      "        In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
      "    \n",
      "    Args:\n",
      "        query (Tensor): Query tensor; shape :math:`(N, ..., Hq, L, E)`.\n",
      "        key (Tensor): Key tensor; shape :math:`(N, ..., H, S, E)`.\n",
      "        value (Tensor): Value tensor; shape :math:`(N, ..., H, S, Ev)`.\n",
      "        attn_mask (optional Tensor): Attention mask; shape must be broadcastable to the shape of attention weights,\n",
      "            which is :math:`(N,..., L, S)`. Two types of masks are supported.\n",
      "            A boolean mask where a value of True indicates that the element *should* take part in attention.\n",
      "            A float mask of the same type as query, key, value that is added to the attention score.\n",
      "        dropout_p (float): Dropout probability; if greater than 0.0, dropout is applied\n",
      "        is_causal (bool): If set to true, the attention masking is a lower triangular matrix when the mask is a\n",
      "            square matrix. The attention masking has the form of the upper left causal bias due to the alignment\n",
      "            (see :class:`torch.nn.attention.bias.CausalBias`) when the mask is a non-square matrix.\n",
      "            An error is thrown if both attn_mask and is_causal are set.\n",
      "        scale (optional float, keyword-only): Scaling factor applied prior to softmax. If None, the default value is set\n",
      "            to :math:`\\frac{1}{\\sqrt{E}}`.\n",
      "        enable_gqa (bool): If set to True, Grouped Query Attention (GQA) is enabled, by default it is set to False.\n",
      "\n",
      "    Returns:\n",
      "        output (Tensor): Attention output; shape :math:`(N, ..., Hq, L, Ev)`.\n",
      "\n",
      "    Shape legend:\n",
      "        - :math:`N: \\text{Batch size} ... : \\text{Any number of other batch dimensions (optional)}`\n",
      "        - :math:`S: \\text{Source sequence length}`\n",
      "        - :math:`L: \\text{Target sequence length}`\n",
      "        - :math:`E: \\text{Embedding dimension of the query and key}`\n",
      "        - :math:`Ev: \\text{Embedding dimension of the value}`\n",
      "        - :math:`Hq: \\text{Number of heads of query}`\n",
      "        - :math:`H: \\text{Number of heads of key and value}`\n",
      "\n",
      "    Examples:\n",
      "\n",
      "        >>> # Optionally use the context manager to ensure one of the fused kernels is run\n",
      "        >>> query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> with sdpa_kernel(backends=[SDPBackend.FLASH_ATTENTION]):\n",
      "        >>>     F.scaled_dot_product_attention(query,key,value)\n",
      "\n",
      "\n",
      "        >>> # Sample for GQA for llama3\n",
      "        >>> query = torch.rand(32, 32, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> with sdpa_kernel(backends=[SDPBackend.MATH]):\n",
      "        >>>     F.scaled_dot_product_attention(query,key,value,enable_gqa=True)\n",
      "\n",
      "\n",
      "    .. _FlashAttention-2\\: Faster Attention with Better Parallelism and Work Partitioning:\n",
      "        https://arxiv.org/abs/2307.08691\n",
      "    .. _Memory-Efficient Attention:\n",
      "        https://github.com/facebookresearch/xformers\n",
      "    .. _Grouped-Query Attention:\n",
      "        https://arxiv.org/pdf/2305.13245\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.nn.functional.scaled_dot_product_attention.__doc__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b72626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.train_comparison import *\n",
    "from utils.processing import image_transform\n",
    "from utils.data.chexpert_dataset import CheXpertDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42455fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPT2 tokenizer.\n"
     ]
    }
   ],
   "source": [
    "tok = build_tokenizer_from_labels(gpt2=True)\n",
    "pad_id = tok.pad_token_id\n",
    "eos_id = tok.eos_token_id\n",
    "bos_id = tok.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19eeaa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering rows with missing PNGs...\n",
      "[INFO] Kept 19/63 rows with existing PNGs\n"
     ]
    }
   ],
   "source": [
    "from utils.data.dataloaders import create_dataloaders\n",
    "\n",
    "# CheXpert\n",
    "CHEXPERT_DIR = \"Datasets/CheXpertPlus\"\n",
    "chexpert_paths = {\n",
    "    \"chexpert_data_path\": f\"{CHEXPERT_DIR}/PNG\",  # base PNG folder\n",
    "    \"chexpert_data_csv\": f\"{CHEXPERT_DIR}/df_chexpert_plus_240401_findings.csv\",\n",
    "}\n",
    "\n",
    "# MIMIC\n",
    "MIMIC_DIR = \"Datasets/MIMIC\"\n",
    "mimic_paths = {\n",
    "    \"mimic_data_path\": MIMIC_DIR,\n",
    "    \"mimic_splits_csv\": f\"{MIMIC_DIR}/mimic-cxr-2.0.0-split.csv.gz\",\n",
    "    \"mimic_metadata_csv\": f\"{MIMIC_DIR}/mimic-cxr-2.0.0-metadata-findings-only.csv\",\n",
    "    \"mimic_reports_path\": f\"{MIMIC_DIR}/cxr-record-list.csv.gz\",  # must contain 'path'\n",
    "    \"mimic_images_dir\": f\"{MIMIC_DIR}/matched_images_and_masks_mimic_224/images\",\n",
    "}\n",
    "\n",
    "import os\n",
    "kwargs = {\n",
    "    # \"num_workers\": os.cpu_count() // 2 if os.cpu_count() else 4,  # adjust on your VM\n",
    "    # \"persistent_workers\": True,           # reuses workers between iterations\n",
    "    # \"prefetch_factor\": 4,                 # each worker prefetches batches\n",
    "    # \"pin_memory\": True,                   # if using CUDA\n",
    "    # \"drop_last\": False\n",
    "}\n",
    "\n",
    "test_loader = create_dataloaders(\n",
    "    chexpert_paths, \n",
    "    mimic_paths, \n",
    "    batch_size=4,\n",
    "    split=\"test\", \n",
    "    sampling_ratio=0.7,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c438d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded segmenter weights from models/dino_unet_decoder_finetuned.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "Overall results for model trained 100 epochs:\n",
      "chexbert_f1_weighted: 0.22205567772508789\n",
      "chexbert_f1_micro: 0.29654100272055967\n",
      "chexbert_f1_macro: 0.14572258507884892\n",
      "chexbert_f1_micro_5: 0.4147615937295885\n",
      "chexbert_f1_macro_5: 0.29443589263681913\n",
      "radgraph_f1_RG_E: 0.20190294429311714\n",
      "radgraph_f1_RG_ER: 0.16685646459235928\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.text_metrics import evaluate_all_metrics, save_metrics_to_json\n",
    "# Load weights directly to DEVICE\n",
    "from utils.models.complete_model import create_complete_model, load_complete_model\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEGMENTER_MODEL_PATH=\"models/dino_unet_decoder_finetuned.pth\"\n",
    "model = create_complete_model(device=DEVICE, SEGMENTER_MODEL_PATH=SEGMENTER_MODEL_PATH)\n",
    "best_model_path = \"checkpoints/model_best.pth\"\n",
    "ckpt = torch.load(best_model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "generated_text, target_text = [], []\n",
    "iteration = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for pixel_values, ids_loader, paths, raw_labels in test_loader:\n",
    "        iteration += 1\n",
    "        \n",
    "        pixel_values = pixel_values.to(model.device, non_blocking=True)\n",
    "\n",
    "        # Visual path\n",
    "        patches = model.encoder(pixel_values)                           # [B,Np,Cenc]\n",
    "        projected_patches = model.linear_projection(patches)            # [B,Np,n_embd]\n",
    "\n",
    "        # Segmentation path per layer\n",
    "        segmented_layers = model.segmenter(pixel_values, model.num_layers) # [B,n_layers,H,W] (per current decoder)\n",
    "\n",
    "\n",
    "        # Generate (disable all plotting/diagnostics for speed)\n",
    "        gen_ids = model.decoder.generate(\n",
    "            inputs_embeds=projected_patches,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=eos_id,\n",
    "            pad_token_id=pad_id,\n",
    "            use_cache=True,\n",
    "            segmentation_mask=segmented_layers,\n",
    "            prefix_allowed_length=0,\n",
    "            plot_attention_mask=False,\n",
    "            plot_attention_mask_layer=[],\n",
    "            plot_attention_map=False,\n",
    "            plot_attention_map_layer=[],\n",
    "            plot_attention_map_generation=0,\n",
    "        )\n",
    "        # Move only the ids needed for decoding to CPU\n",
    "        texts = model.tokenizer.batch_decode(gen_ids.detach().cpu(), skip_special_tokens=True)\n",
    "\n",
    "        # Accumulate for final metric pass (metrics often run on CPU/strings anyway)\n",
    "        generated_text.extend(texts)\n",
    "        target_text.extend(ids_loader)\n",
    "\n",
    "        if iteration >= 200:  # your test cap\n",
    "            break\n",
    "\n",
    "# Evaluate once per model\n",
    "eval_results = evaluate_all_metrics(\n",
    "    generated=generated_text,\n",
    "    original=target_text,\n",
    "    evaluation_mode=\"CheXagent\"\n",
    ")\n",
    "\n",
    "print(f\"\\nOverall results for model trained {100} epochs:\")\n",
    "for metric, scores in eval_results.items():\n",
    "    print(f\"{metric}: {scores}\")\n",
    "\n",
    "# add training walltime you tracked\n",
    "eval_results[\"training_time_seconds\"] = 3600*7\n",
    "\n",
    "# # Save metrics\n",
    "# save_metrics_to_json(\n",
    "#     eval_results,\n",
    "#     f\"lstm-vs-gpt/results/cloud_best_model_{100}_Chexpert.json\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47998dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-chest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
